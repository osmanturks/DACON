{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt # 데이터 시각화\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import os\n",
    "import re\n",
    "import multiprocessing # 여러 개의 일꾼 (cpu)들에게 작업을 분산시키는 역할\n",
    "from multiprocessing import Pool \n",
    "from functools import partial # 함수가 받는 인자들 중 몇개를 고정 시켜서 새롭게 파생된 함수를 형성하는 역할\n",
    "from data_loader_v2 import data_loader_v2\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (13,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "import lightgbm as lgb\n",
    "seed = 777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_label = pd.read_csv(\"train_label.csv\")\n",
    "plt.hist(train_label['label'], bins=len(train_label['label'].unique()))\n",
    "plt.xlim(0,200)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('# of label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체적으로 Imbalance Data, 1개뿐인 라벨이 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad, CLOSE, Equip Fail, No Data, Normal, OFF, ON, OPEN, System.Char[] 등 문자열 데이터를 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "realData1 = \"Train/30.csv\"\n",
    "realData2 = \"Test/1154.csv\"\n",
    "realData3 = \"Test/1168.csv\"\n",
    "realData4 = \"additinal_data/additinal_data1\"\n",
    "realData5 = \"additinal_data/additinal_data2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'train/'\n",
    "test_folder = 'test/'\n",
    "train_label_path = 'train_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = os.listdir(train_folder)\n",
    "test_list = os.listdir(test_folder)\n",
    "train_label = pd.read_csv(train_label_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataPreprocessn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 csv 파일의 상태_B로 변화는 시점이 10초로 가정\n",
    "def data_loader_v2_all(func, files, folder='', train_label=None, event_time=10, nrows=60):   \n",
    "    func_fixed = partial(func, folder=folder, train_label=train_label, event_time=event_time, nrows=nrows)     \n",
    "    if __name__ == '__main__':\n",
    "        pool = Pool(processes=multiprocessing.cpu_count()) \n",
    "        df_list = list(pool.imap(func_fixed, files)) \n",
    "        pool.close()\n",
    "        pool.join()        \n",
    "    combined_df = pd.concat(df_list)    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = data_loader_v2_all(data_loader_v2, train_list, folder=train_folder, train_label=train_label, event_time=10, nrows=60)\n",
    "#train.to_csv(\"base_train.csv\")\n",
    "train = pd.read_csv(\"base_train.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 test 셋 데이터에 대해서 10초부터 상태_B가 시작된다고 가정 \n",
    "#test = data_loader_v2_all(data_loader_v2, test_list, folder=test_folder, train_label=None, event_time=10, nrows=60)\n",
    "#test.to_csv(\"base_test.csv\")\n",
    "test = pd.read_csv(\"base_test.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c = train.copy()\n",
    "train_label = train_c['label']\n",
    "train_data = train_c.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         V0000     V0001     V0002     V0003     V0004       V0005  \\\n",
      "666  30.460902  8.696729  8.678150  8.707048  8.743941  187.271493   \n",
      "675  30.467751  8.634908  8.721089  8.677828  8.720551  208.165981   \n",
      "547  30.473759  8.729892  8.689503  8.666899  8.672289  253.143294   \n",
      "248  30.484401  8.759098  8.716958  8.730716  8.687358  178.700974   \n",
      "389  30.469542  8.785799  8.717737  8.735726  8.717749  176.454884   \n",
      "..         ...       ...       ...       ...       ...         ...   \n",
      "811  23.635265  4.311902  4.222889  3.817453  3.780163  -23.022390   \n",
      "317  30.477458  8.750068  8.722127  8.723330  8.707557  196.777031   \n",
      "589  30.470703  8.831234  8.686700  8.706548  8.714215  187.066875   \n",
      "819  30.446290  8.736160  8.680907  8.678161  8.692547  279.684095   \n",
      "525  30.476703  8.689716  8.664188  8.677084  8.653015  239.878768   \n",
      "\n",
      "          V0006         V0007  V0008       V0009  ...  V5111  V5112  V5113  \\\n",
      "666  220.187777 -2.609061e-19    0.0   -0.000446  ...    1.0    1.0    1.0   \n",
      "675  261.808273 -2.894820e-19    0.0   -0.000780  ...    1.0    1.0    1.0   \n",
      "547  227.703277  3.146973e-19    0.0    0.001592  ...    1.0    1.0    1.0   \n",
      "248  184.457616  4.747937e-19    0.0    0.000507  ...    1.0    1.0    1.0   \n",
      "389  113.840214 -2.867486e-19    0.0   -0.000513  ...    1.0    1.0    1.0   \n",
      "..          ...           ...    ...         ...  ...    ...    ...    ...   \n",
      "811   27.120105  2.651098e+02    0.0  261.567807  ...    1.0    1.0    1.0   \n",
      "317  196.206859  7.336516e-19    0.0    0.000211  ...    1.0    1.0    1.0   \n",
      "589  188.170848  4.909930e-20    0.0   -0.000879  ...    1.0    1.0    1.0   \n",
      "819  263.235703 -4.892852e-19    0.0    0.002271  ...    1.0    1.0    1.0   \n",
      "525  265.317991  6.309064e-19    0.0    0.002161  ...    1.0    1.0    1.0   \n",
      "\n",
      "     V5114  V5115  V5116  V5117      V5118     V5119  V5120  \n",
      "666    1.0   60.0    0.0    0.0  -0.000003  85.40000    0.0  \n",
      "675    1.0   60.0    0.0    0.0  -0.000003  85.40000    0.0  \n",
      "547    1.0   60.0    0.0    0.0  -0.000012  85.40000    0.0  \n",
      "248    1.0   60.0    0.0    0.0   0.000008  85.40000    0.0  \n",
      "389    1.0   60.0    0.0    0.0   0.000020  85.40000    0.0  \n",
      "..     ...    ...    ...    ...        ...       ...    ...  \n",
      "811    1.0    0.1    0.0    0.0  70.678238   8.82437    0.0  \n",
      "317    1.0   60.0    0.0    0.0  -0.000009  85.40000    0.0  \n",
      "589    1.0   60.0    0.0    0.0  -0.000002  85.40000    0.0  \n",
      "819    1.0   60.0    0.0    0.0  -0.000004  85.40000    0.0  \n",
      "525    1.0   60.0    0.0    0.0  -0.000015  85.40000    0.0  \n",
      "\n",
      "[33120 rows x 5121 columns]          V0000     V0001     V0002     V0003     V0004       V0005  \\\n",
      "290  30.466830  8.544912  8.474796  8.495731  8.463197  268.175079   \n",
      "588  30.475909  8.791712  8.705517  8.661790  8.690852  240.634524   \n",
      "359  30.489189  8.626852  8.545856  8.695577  8.689374  213.208885   \n",
      "204  30.480330  8.740064  8.721607  8.703408  8.719145  155.095346   \n",
      "107  30.485783  8.664525  8.710334  8.707884  8.714969  197.504720   \n",
      "..         ...       ...       ...       ...       ...         ...   \n",
      "60   30.456247  8.836413  8.716750  8.674596  8.705695  173.506788   \n",
      "319  30.465355  8.790987  8.689226  8.718441  8.735292  165.127928   \n",
      "220  30.483190  8.836461  8.704230  8.730588  8.711972  190.621019   \n",
      "707  30.473985  8.622354  8.683352  8.689707  8.698356  178.334101   \n",
      "302  30.488181  8.581986  8.478567  8.476242  8.520133  261.774746   \n",
      "\n",
      "          V0006         V0007      V0008       V0009  ...  V5111  V5112  \\\n",
      "290  276.259286  2.613470e+02  261.00864  261.962360  ...    1.0    1.0   \n",
      "588  197.651943  1.634715e-20    0.00000    0.000274  ...    1.0    1.0   \n",
      "359  220.488462  5.043719e-19    0.00000    0.000115  ...    1.0    1.0   \n",
      "204  168.565382  4.265669e-19    0.00000   -0.000768  ...    1.0    1.0   \n",
      "107  160.205515 -4.186459e-20    0.00000   -0.002249  ...    1.0    1.0   \n",
      "..          ...           ...        ...         ...  ...    ...    ...   \n",
      "60   200.769245  4.099004e-19    0.00000    0.001423  ...    1.0    1.0   \n",
      "319  176.782001  1.694838e-19    0.00000    0.002014  ...    1.0    1.0   \n",
      "220  211.138741  3.666355e-19    0.00000    0.000201  ...    1.0    1.0   \n",
      "707  151.790755 -4.695022e-19    0.00000    0.000922  ...    1.0    1.0   \n",
      "302  279.461111  4.948041e-19    0.00000  264.649532  ...    1.0    1.0   \n",
      "\n",
      "     V5113  V5114    V5115  V5116  V5117         V5118     V5119  V5120  \n",
      "290    1.0    1.0  60.0000    0.0    0.0  5.866493e-06  85.40000    0.0  \n",
      "588    1.0    1.0  60.0000    0.0    0.0  7.471110e-06  85.40000    0.0  \n",
      "359    1.0    1.0  60.0000    0.0    0.0 -7.269697e-07  85.40000    0.0  \n",
      "204    1.0    1.0  60.0000    0.0    0.0 -8.129589e-06  85.40000    0.0  \n",
      "107    1.0    1.0  60.0000    0.0    0.0 -8.963084e-06  85.40000    0.0  \n",
      "..     ...    ...      ...    ...    ...           ...       ...    ...  \n",
      "60     1.0    1.0  60.0000    0.0    0.0  9.646324e-06  85.40000    0.0  \n",
      "319    1.0    1.0  60.0000    0.0    0.0 -2.462708e-05  85.40000    0.0  \n",
      "220    1.0    1.0  60.0000    0.0    0.0 -1.072581e-05  85.40000    0.0  \n",
      "707    1.0    1.0  60.0000    0.0    0.0 -2.109555e-06  85.40000    0.0  \n",
      "302    1.0    1.0  59.8849    0.0    0.0  1.083335e-05   6.12653    0.0  \n",
      "\n",
      "[8280 rows x 5121 columns] 666     74\n",
      "675    112\n",
      "547    155\n",
      "248    195\n",
      "389    148\n",
      "      ... \n",
      "811     53\n",
      "317    114\n",
      "589    179\n",
      "819     38\n",
      "525     38\n",
      "Name: label, Length: 33120, dtype: int64 290      0\n",
      "588    156\n",
      "359     25\n",
      "204    169\n",
      "107    111\n",
      "      ... \n",
      "60     172\n",
      "319    122\n",
      "220    119\n",
      "707     68\n",
      "302     49\n",
      "Name: label, Length: 8280, dtype: int64\n",
      "(33120, 5121) (8280, 5121) (33120,) (8280,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(train_1, train_y, test_size=0.2, random_state=seed)\n",
    "print(X_train,X_test,Y_train,Y_test)\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustScaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_train_1 = pd.DataFrame(robustScaler.fit_transform(train_1), columns=train_1.columns, index = train_1.index.values)\n",
    "rob_test = pd.DataFrame(robustScaler.fit_transform(test), columns=test.columns, index = test.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          V0000      V0001       V0002       V0003       V0004     V0005  \\\n",
      "666   -0.686373  -0.258664   -0.966421    0.070733    1.182847 -0.324421   \n",
      "675   -0.259886  -0.833324    0.597189   -0.600461    0.460626  0.558946   \n",
      "547    0.114254   0.049602   -0.553022   -0.851495   -1.029616  2.460476   \n",
      "248    0.776996   0.321080    0.446751    0.614380   -0.564326 -0.686762   \n",
      "389   -0.148344   0.569288    0.475110    0.729478    0.374098 -0.781721   \n",
      "..          ...        ...         ...         ...         ...       ...   \n",
      "811 -425.761128 -41.017939 -163.205259 -112.244763 -152.089454 -9.215128   \n",
      "317    0.344627   0.237146    0.634973    0.444730    0.059377  0.077449   \n",
      "589   -0.076033   0.991626   -0.655079    0.059241    0.264972 -0.333072   \n",
      "819   -1.596356   0.107864   -0.866058   -0.592821   -0.404099  3.582555   \n",
      "525    0.297606  -0.323860   -1.474858   -0.617546   -1.624753  1.899684   \n",
      "\n",
      "        V0006         V0007  V0008          V0009  ...  V5111  V5112  V5113  \\\n",
      "666  0.605048 -4.870297e-01    0.0      -0.265141  ...    0.0    0.0    0.0   \n",
      "675  1.598190 -5.368594e-01    0.0      -0.438751  ...    0.0    0.0    0.0   \n",
      "547  0.784382  5.166891e-01    0.0       0.792162  ...    0.0    0.0    0.0   \n",
      "248 -0.247541  7.958602e-01    0.0       0.229336  ...    0.0    0.0    0.0   \n",
      "389 -1.932604 -5.320931e-01    0.0      -0.299854  ...    0.0    0.0    0.0   \n",
      "..        ...           ...    ...            ...  ...    ...    ...    ...   \n",
      "811 -4.001907  4.622899e+20    0.0  135709.273844  ...    0.0    0.0    0.0   \n",
      "317  0.032818  1.247248e+00    0.0       0.075394  ...    0.0    0.0    0.0   \n",
      "589 -0.158936  5.354783e-02    0.0      -0.490191  ...    0.0    0.0    0.0   \n",
      "819  1.632252 -8.852698e-01    0.0       1.144353  ...    0.0    0.0    0.0   \n",
      "525  1.681939  1.068085e+00    0.0       1.087165  ...    0.0    0.0    0.0   \n",
      "\n",
      "     V5114  V5115  V5116  V5117         V5118     V5119  V5120  \n",
      "666    0.0    0.0    0.0    0.0 -1.737990e-01   0.00000    0.0  \n",
      "675    0.0    0.0    0.0    0.0 -1.935165e-01   0.00000    0.0  \n",
      "547    0.0    0.0    0.0    0.0 -6.897701e-01   0.00000    0.0  \n",
      "248    0.0    0.0    0.0    0.0  4.253374e-01   0.00000    0.0  \n",
      "389    0.0    0.0    0.0    0.0  1.082608e+00   0.00000    0.0  \n",
      "..     ...    ...    ...    ...           ...       ...    ...  \n",
      "811    0.0  -59.9    0.0    0.0  4.002746e+06 -76.57563    0.0  \n",
      "317    0.0    0.0    0.0    0.0 -5.467140e-01   0.00000    0.0  \n",
      "589    0.0    0.0    0.0    0.0 -1.514971e-01   0.00000    0.0  \n",
      "819    0.0    0.0    0.0    0.0 -2.663053e-01   0.00000    0.0  \n",
      "525    0.0    0.0    0.0    0.0 -8.478886e-01   0.00000    0.0  \n",
      "\n",
      "[33120 rows x 5121 columns]         V0000     V0001     V0002     V0003     V0004     V0005     V0006  \\\n",
      "290 -0.317247 -1.669886 -8.371611 -4.783286 -7.486005  3.095983  1.943019   \n",
      "588  0.248161  0.624247  0.030134 -0.968857 -0.456415  1.931636  0.067301   \n",
      "359  1.075180 -0.908210 -5.783947 -0.192755 -0.502065  0.772148  0.612223   \n",
      "204  0.523483  0.144158  0.616060 -0.012872  0.417194 -1.684749 -0.626759   \n",
      "107  0.863072 -0.558023  0.205539  0.089941  0.288273  0.108214 -0.826241   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "60  -0.976279  1.039765  0.439176 -0.674692  0.001896 -0.906359  0.141685   \n",
      "319 -0.409113  0.617512 -0.563117  0.332442  0.915793 -1.260597 -0.430695   \n",
      "220  0.701609  1.040217 -0.016715  0.611442  0.195729 -0.182812  0.389121   \n",
      "707  0.128385 -0.950026 -0.777023 -0.327587 -0.224735 -0.702272 -1.027033   \n",
      "302  1.012438 -1.325266 -8.234265 -5.230949 -5.727907  2.825392  2.019420   \n",
      "\n",
      "            V0007      V0008          V0009  ...  V5111  V5112  V5113  V5114  \\\n",
      "290  4.557286e+20  261.00864  135913.979917  ...    0.0    0.0    0.0    0.0   \n",
      "588 -3.564319e-03    0.00000       0.108053  ...    0.0    0.0    0.0    0.0   \n",
      "359  8.474376e-01    0.00000       0.026038  ...    0.0    0.0    0.0    0.0   \n",
      "204  7.117637e-01    0.00000      -0.432536  ...    0.0    0.0    0.0    0.0   \n",
      "107 -1.050721e-01    0.00000      -1.200933  ...    0.0    0.0    0.0    0.0   \n",
      "..            ...        ...            ...  ...    ...    ...    ...    ...   \n",
      "60   6.827012e-01    0.00000       0.704331  ...    0.0    0.0    0.0    0.0   \n",
      "319  2.634705e-01    0.00000       1.010966  ...    0.0    0.0    0.0    0.0   \n",
      "220  6.072573e-01    0.00000       0.070294  ...    0.0    0.0    0.0    0.0   \n",
      "707 -8.507730e-01    0.00000       0.444675  ...    0.0    0.0    0.0    0.0   \n",
      "302  8.307536e-01    0.00000  137308.166206  ...    0.0    0.0    0.0    0.0   \n",
      "\n",
      "      V5115  V5116  V5117     V5118     V5119  V5120  \n",
      "290  0.0000    0.0    0.0  0.309587   0.00000    0.0  \n",
      "588  0.0000    0.0    0.0  0.400461   0.00000    0.0  \n",
      "359  0.0000    0.0    0.0 -0.063823   0.00000    0.0  \n",
      "204  0.0000    0.0    0.0 -0.483059   0.00000    0.0  \n",
      "107  0.0000    0.0    0.0 -0.530262   0.00000    0.0  \n",
      "..      ...    ...    ...       ...       ...    ...  \n",
      "60   0.0000    0.0    0.0  0.523651   0.00000    0.0  \n",
      "319  0.0000    0.0    0.0 -1.417367   0.00000    0.0  \n",
      "220  0.0000    0.0    0.0 -0.630091   0.00000    0.0  \n",
      "707  0.0000    0.0    0.0 -0.142124   0.00000    0.0  \n",
      "302 -0.1151    0.0    0.0  0.590876 -79.27347    0.0  \n",
      "\n",
      "[8280 rows x 5121 columns] 666     74\n",
      "675    112\n",
      "547    155\n",
      "248    195\n",
      "389    148\n",
      "      ... \n",
      "811     53\n",
      "317    114\n",
      "589    179\n",
      "819     38\n",
      "525     38\n",
      "Name: label, Length: 33120, dtype: int64 290      0\n",
      "588    156\n",
      "359     25\n",
      "204    169\n",
      "107    111\n",
      "      ... \n",
      "60     172\n",
      "319    122\n",
      "220    119\n",
      "707     68\n",
      "302     49\n",
      "Name: label, Length: 8280, dtype: int64\n",
      "(33120, 5121) (8280, 5121) (33120,) (8280,)\n"
     ]
    }
   ],
   "source": [
    "r_X_train, r_X_test, r_Y_train, r_Y_test = train_test_split(rob_train_1, train_y, test_size=0.2, random_state=seed)\n",
    "print(r_X_train,r_X_test,r_Y_train,r_Y_test)\n",
    "print(r_X_train.shape,r_X_test.shape,r_Y_train.shape,r_Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClaasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=-1, oob_score=False, random_state=0, verbose=1,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=0, verbose=1, n_jobs=-1)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        20\n",
      "           1       1.00      0.47      0.64        17\n",
      "           2       0.95      0.82      0.88        22\n",
      "           3       0.91      0.80      0.85        25\n",
      "           4       0.96      0.71      0.82        38\n",
      "           5       1.00      0.85      0.92        41\n",
      "           6       1.00      0.92      0.96        37\n",
      "           7       1.00      0.93      0.96        28\n",
      "           8       0.68      0.92      0.78       125\n",
      "           9       0.23      0.11      0.15        28\n",
      "          10       0.33      0.75      0.46        32\n",
      "          11       0.00      0.00      0.00        22\n",
      "          12       0.88      0.28      0.42        25\n",
      "          13       1.00      0.06      0.11        35\n",
      "          14       0.25      0.27      0.26        30\n",
      "          15       0.60      0.56      0.58        16\n",
      "          16       0.40      0.29      0.33        35\n",
      "          17       0.72      0.95      0.82       216\n",
      "          18       0.99      0.97      0.98       105\n",
      "          19       0.80      0.84      0.82        51\n",
      "          20       0.71      0.43      0.54        23\n",
      "          21       0.98      1.00      0.99        56\n",
      "          22       1.00      0.97      0.99        40\n",
      "          23       0.85      0.88      0.86        76\n",
      "          24       1.00      0.86      0.93        22\n",
      "          25       1.00      0.83      0.91        30\n",
      "          26       1.00      0.93      0.96        27\n",
      "          27       1.00      0.89      0.94        28\n",
      "          28       0.88      0.93      0.91       180\n",
      "          29       0.94      0.89      0.92        57\n",
      "          30       1.00      0.96      0.98        56\n",
      "          31       1.00      1.00      1.00         7\n",
      "          32       0.86      0.86      0.86         7\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.93      0.88      0.90        16\n",
      "          35       1.00      0.92      0.96        12\n",
      "          36       1.00      0.27      0.43        11\n",
      "          37       1.00      0.67      0.80        12\n",
      "          38       0.85      0.96      0.90       183\n",
      "          39       1.00      0.89      0.94         9\n",
      "          40       0.98      0.96      0.97        85\n",
      "          41       0.92      1.00      0.96        11\n",
      "          42       0.86      0.75      0.80         8\n",
      "          43       0.83      0.91      0.87        11\n",
      "          44       1.00      0.81      0.90        16\n",
      "          45       0.89      0.89      0.89         9\n",
      "          46       0.88      0.88      0.88         8\n",
      "          47       1.00      0.91      0.95        11\n",
      "          48       0.64      0.78      0.70         9\n",
      "          49       0.83      0.91      0.87        11\n",
      "          50       1.00      0.67      0.80         6\n",
      "          51       0.90      1.00      0.95         9\n",
      "          52       0.73      0.89      0.80         9\n",
      "          53       0.98      0.98      0.98        54\n",
      "          54       0.98      0.93      0.95       112\n",
      "          55       0.80      0.40      0.53        10\n",
      "          56       0.25      0.08      0.12        12\n",
      "          57       0.33      0.08      0.12        13\n",
      "          58       1.00      0.73      0.85        15\n",
      "          59       0.50      0.18      0.27        11\n",
      "          60       0.62      0.50      0.56        10\n",
      "          61       1.00      1.00      1.00        43\n",
      "          62       1.00      0.98      0.99        55\n",
      "          63       0.67      0.58      0.62        31\n",
      "          64       0.69      0.66      0.68        38\n",
      "          65       0.88      0.50      0.64        42\n",
      "          66       0.56      0.96      0.71        23\n",
      "          67       0.47      0.82      0.60        38\n",
      "          68       1.00      0.98      0.99       131\n",
      "          69       0.91      0.92      0.92       157\n",
      "          70       1.00      0.77      0.87        13\n",
      "          71       0.00      0.00      0.00        10\n",
      "          72       0.75      0.50      0.60         6\n",
      "          73       0.90      0.50      0.64        18\n",
      "          74       0.75      0.30      0.43        10\n",
      "          75       0.95      0.89      0.92        83\n",
      "          76       0.87      0.93      0.90        80\n",
      "          77       0.29      0.32      0.31        87\n",
      "          78       0.97      0.90      0.94        40\n",
      "          79       0.81      0.63      0.71        35\n",
      "          80       0.88      0.27      0.41        26\n",
      "          81       1.00      0.52      0.68        27\n",
      "          82       1.00      0.60      0.75        15\n",
      "          83       0.32      0.79      0.46       105\n",
      "          84       1.00      0.95      0.98        22\n",
      "          85       0.92      0.92      0.92        73\n",
      "          86       1.00      0.97      0.98        33\n",
      "          87       1.00      0.88      0.94        25\n",
      "          88       1.00      0.75      0.86        28\n",
      "          89       1.00      0.90      0.95        20\n",
      "          90       1.00      0.85      0.92        26\n",
      "          91       0.68      0.70      0.69       105\n",
      "          92       1.00      0.93      0.96        14\n",
      "          93       1.00      0.82      0.90        11\n",
      "          94       1.00      0.80      0.89        10\n",
      "          95       1.00      1.00      1.00        11\n",
      "          96       1.00      1.00      1.00         8\n",
      "          97       1.00      1.00      1.00         6\n",
      "          98       1.00      0.75      0.86        12\n",
      "          99       1.00      1.00      1.00         6\n",
      "         100       0.92      0.92      0.92        12\n",
      "         101       1.00      0.22      0.36         9\n",
      "         102       0.00      0.00      0.00        10\n",
      "         103       0.00      0.00      0.00        18\n",
      "         104       0.60      0.33      0.43         9\n",
      "         105       0.67      0.14      0.24        14\n",
      "         106       1.00      0.11      0.20         9\n",
      "         107       1.00      0.70      0.82        10\n",
      "         108       0.00      0.00      0.00        12\n",
      "         109       1.00      0.33      0.50        12\n",
      "         110       0.79      0.95      0.86       242\n",
      "         111       0.85      0.93      0.89       192\n",
      "         112       0.92      0.95      0.93       192\n",
      "         113       0.97      0.94      0.95       201\n",
      "         114       0.80      0.94      0.87       206\n",
      "         115       0.98      0.97      0.98       176\n",
      "         116       1.00      0.98      0.99       181\n",
      "         117       1.00      0.95      0.97       182\n",
      "         118       0.86      0.93      0.90       235\n",
      "         119       0.63      0.56      0.59        59\n",
      "         120       0.65      0.59      0.62        44\n",
      "         121       0.84      0.80      0.82        45\n",
      "         122       0.65      0.59      0.62        69\n",
      "         123       0.53      0.57      0.55        61\n",
      "         124       0.90      0.70      0.79        61\n",
      "         125       0.98      0.93      0.95        55\n",
      "         126       0.93      0.95      0.94        93\n",
      "         127       0.95      0.97      0.96        39\n",
      "         128       0.86      0.93      0.89       117\n",
      "         129       0.94      0.92      0.93        49\n",
      "         130       0.90      0.84      0.87       115\n",
      "         131       0.91      0.93      0.92        42\n",
      "         132       0.74      0.97      0.83        89\n",
      "         133       1.00      0.62      0.77         8\n",
      "         134       1.00      0.92      0.96        12\n",
      "         135       1.00      0.78      0.88         9\n",
      "         136       0.97      0.83      0.89        46\n",
      "         137       1.00      0.83      0.91        12\n",
      "         138       1.00      0.91      0.95        11\n",
      "         139       1.00      0.73      0.85        15\n",
      "         140       1.00      0.58      0.74        12\n",
      "         141       1.00      0.91      0.95        11\n",
      "         142       0.60      0.50      0.55         6\n",
      "         143       1.00      0.64      0.78        14\n",
      "         144       1.00      0.46      0.63        13\n",
      "         145       0.88      0.58      0.70        12\n",
      "         146       1.00      0.83      0.91        12\n",
      "         147       0.85      0.83      0.84        47\n",
      "         148       1.00      0.58      0.74        12\n",
      "         149       0.80      1.00      0.89        12\n",
      "         150       1.00      0.50      0.67        12\n",
      "         151       1.00      0.70      0.82        10\n",
      "         152       1.00      0.64      0.78        11\n",
      "         153       1.00      0.67      0.80         9\n",
      "         154       1.00      0.70      0.82        10\n",
      "         155       0.87      0.89      0.88        38\n",
      "         156       0.92      0.90      0.91        39\n",
      "         157       0.94      0.88      0.91        33\n",
      "         158       0.85      0.86      0.85        58\n",
      "         159       1.00      0.82      0.90        17\n",
      "         160       0.94      0.82      0.87        55\n",
      "         161       1.00      0.73      0.85        15\n",
      "         162       0.41      0.92      0.57        60\n",
      "         163       1.00      1.00      1.00        10\n",
      "         164       0.94      0.89      0.91        53\n",
      "         165       0.90      0.66      0.76        41\n",
      "         166       0.78      0.84      0.81        45\n",
      "         167       0.95      0.97      0.96        40\n",
      "         168       0.46      0.82      0.59        50\n",
      "         169       0.65      0.56      0.60        43\n",
      "         170       0.00      0.00      0.00        10\n",
      "         171       0.40      0.25      0.31         8\n",
      "         172       0.93      0.92      0.93        77\n",
      "         173       0.98      0.91      0.94        96\n",
      "         174       0.99      0.89      0.94        93\n",
      "         175       1.00      0.96      0.98        77\n",
      "         176       0.98      0.90      0.94        93\n",
      "         177       0.95      0.91      0.93        89\n",
      "         178       1.00      0.98      0.99        83\n",
      "         179       1.00      0.99      0.99        79\n",
      "         180       1.00      0.92      0.96        83\n",
      "         181       0.93      0.93      0.93        83\n",
      "         182       0.00      0.00      0.00        12\n",
      "         183       1.00      1.00      1.00        11\n",
      "         184       0.92      1.00      0.96        11\n",
      "         185       1.00      0.75      0.86         4\n",
      "         186       1.00      1.00      1.00         5\n",
      "         187       1.00      0.85      0.92        13\n",
      "         188       1.00      1.00      1.00        11\n",
      "         189       1.00      1.00      1.00         7\n",
      "         190       1.00      0.67      0.80        15\n",
      "         191       1.00      1.00      1.00        10\n",
      "         192       0.91      0.91      0.91        23\n",
      "         193       1.00      0.74      0.85        23\n",
      "         194       1.00      0.62      0.76        13\n",
      "         195       1.00      0.50      0.67        10\n",
      "         196       0.45      0.71      0.56         7\n",
      "         197       1.00      0.07      0.12        15\n",
      "\n",
      "    accuracy                           0.84      8602\n",
      "   macro avg       0.85      0.74      0.77      8602\n",
      "weighted avg       0.86      0.84      0.84      8602\n",
      "\n",
      "\n",
      "[[16  0  0 ...  0  0  0]\n",
      " [ 0  8  1 ...  0  0  0]\n",
      " [ 0  0 18 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  5  1  0]\n",
      " [ 0  0  0 ...  0  5  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "expected_y  = Y_test\n",
    "predicted_y = model.predict(X_test).round()\n",
    "print(); print('RandomForestClassifier: ')\n",
    "print(); print(metrics.classification_report(expected_y, predicted_y))\n",
    "print(); print(metrics.confusion_matrix(expected_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.5min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    4.8s finished\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=0, verbose=1, n_jobs=-1)\n",
    "model.fit(rob_train_1, train_y)\n",
    "pred = model.predict_proba(rob_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data=pred)\n",
    "submission.index = test.index\n",
    "submission.index.name = 'id'\n",
    "submission = submission.sort_index()\n",
    "submission = submission.groupby('id').mean()\n",
    "submission.to_csv('submission_baseline_rob.csv', index=True) #제출 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-c882ced9a0e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "train_1.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train_1.columns]\n",
    "\n",
    "X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "X_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n",
    "\n",
    "train_ds = lgb.Dataset(X_train, label = Y_train)\n",
    "test_ds = lgb.Dataset(X_test, label = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "#model.fit(X_train,Y_train)\n",
    "lgb_params = {\n",
    "        'objective':'multiclass',\n",
    "        'metric':'multi_logloss',\n",
    "        'num_class':198,\n",
    "        'learning_rate':0.02,\n",
    "        'seed': 5,\n",
    "        'n_jobs': -1\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 3.28624\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's multi_logloss: 3.02749\n",
      "[3]\tvalid_0's multi_logloss: 2.84508\n",
      "[4]\tvalid_0's multi_logloss: 2.69981\n",
      "[5]\tvalid_0's multi_logloss: 2.57696\n",
      "[6]\tvalid_0's multi_logloss: 2.47148\n",
      "[7]\tvalid_0's multi_logloss: 2.37878\n",
      "[8]\tvalid_0's multi_logloss: 2.29634\n",
      "[9]\tvalid_0's multi_logloss: 2.22167\n",
      "[10]\tvalid_0's multi_logloss: 2.15358\n",
      "[11]\tvalid_0's multi_logloss: 2.09138\n",
      "[12]\tvalid_0's multi_logloss: 2.03358\n",
      "[13]\tvalid_0's multi_logloss: 1.97991\n",
      "[14]\tvalid_0's multi_logloss: 1.92964\n",
      "[15]\tvalid_0's multi_logloss: 1.88244\n",
      "[16]\tvalid_0's multi_logloss: 1.83787\n",
      "[17]\tvalid_0's multi_logloss: 1.79593\n",
      "[18]\tvalid_0's multi_logloss: 1.75591\n",
      "[19]\tvalid_0's multi_logloss: 1.718\n",
      "[20]\tvalid_0's multi_logloss: 1.68209\n",
      "[21]\tvalid_0's multi_logloss: 1.64778\n",
      "[22]\tvalid_0's multi_logloss: 1.61461\n",
      "[23]\tvalid_0's multi_logloss: 1.58296\n",
      "[24]\tvalid_0's multi_logloss: 1.55287\n",
      "[25]\tvalid_0's multi_logloss: 1.5239\n",
      "[26]\tvalid_0's multi_logloss: 1.49611\n",
      "[27]\tvalid_0's multi_logloss: 1.46939\n",
      "[28]\tvalid_0's multi_logloss: 1.4435\n",
      "[29]\tvalid_0's multi_logloss: 1.41856\n",
      "[30]\tvalid_0's multi_logloss: 1.39445\n",
      "[31]\tvalid_0's multi_logloss: 1.37117\n",
      "[32]\tvalid_0's multi_logloss: 1.34856\n",
      "[33]\tvalid_0's multi_logloss: 1.32662\n",
      "[34]\tvalid_0's multi_logloss: 1.30536\n",
      "[35]\tvalid_0's multi_logloss: 1.28497\n",
      "[36]\tvalid_0's multi_logloss: 1.26503\n",
      "[37]\tvalid_0's multi_logloss: 1.24566\n",
      "[38]\tvalid_0's multi_logloss: 1.22686\n",
      "[39]\tvalid_0's multi_logloss: 1.20864\n",
      "[40]\tvalid_0's multi_logloss: 1.19093\n",
      "[41]\tvalid_0's multi_logloss: 1.17367\n",
      "[42]\tvalid_0's multi_logloss: 1.15688\n",
      "[43]\tvalid_0's multi_logloss: 1.14054\n",
      "[44]\tvalid_0's multi_logloss: 1.12466\n",
      "[45]\tvalid_0's multi_logloss: 1.10934\n",
      "[46]\tvalid_0's multi_logloss: 1.0943\n",
      "[47]\tvalid_0's multi_logloss: 1.07961\n",
      "[48]\tvalid_0's multi_logloss: 1.06532\n",
      "[49]\tvalid_0's multi_logloss: 1.0513\n",
      "[50]\tvalid_0's multi_logloss: 1.03776\n",
      "[51]\tvalid_0's multi_logloss: 1.02442\n",
      "[52]\tvalid_0's multi_logloss: 1.0114\n",
      "[53]\tvalid_0's multi_logloss: 0.998729\n",
      "[54]\tvalid_0's multi_logloss: 0.986405\n",
      "[55]\tvalid_0's multi_logloss: 0.974285\n",
      "[56]\tvalid_0's multi_logloss: 0.962444\n",
      "[57]\tvalid_0's multi_logloss: 0.950977\n",
      "[58]\tvalid_0's multi_logloss: 0.93965\n",
      "[59]\tvalid_0's multi_logloss: 0.928659\n",
      "[60]\tvalid_0's multi_logloss: 0.917885\n",
      "[61]\tvalid_0's multi_logloss: 0.907379\n",
      "[62]\tvalid_0's multi_logloss: 0.89703\n",
      "[63]\tvalid_0's multi_logloss: 0.886962\n",
      "[64]\tvalid_0's multi_logloss: 0.877121\n",
      "[65]\tvalid_0's multi_logloss: 0.867564\n",
      "[66]\tvalid_0's multi_logloss: 0.858152\n",
      "[67]\tvalid_0's multi_logloss: 0.848929\n",
      "[68]\tvalid_0's multi_logloss: 0.839946\n",
      "[69]\tvalid_0's multi_logloss: 0.831047\n",
      "[70]\tvalid_0's multi_logloss: 0.82244\n",
      "[71]\tvalid_0's multi_logloss: 0.813973\n",
      "[72]\tvalid_0's multi_logloss: 0.80574\n",
      "[73]\tvalid_0's multi_logloss: 0.797593\n",
      "[74]\tvalid_0's multi_logloss: 0.789541\n",
      "[75]\tvalid_0's multi_logloss: 0.781708\n",
      "[76]\tvalid_0's multi_logloss: 0.774058\n",
      "[77]\tvalid_0's multi_logloss: 0.766586\n",
      "[78]\tvalid_0's multi_logloss: 0.759244\n",
      "[79]\tvalid_0's multi_logloss: 0.752037\n",
      "[80]\tvalid_0's multi_logloss: 0.744956\n",
      "[81]\tvalid_0's multi_logloss: 0.73802\n",
      "[82]\tvalid_0's multi_logloss: 0.731242\n",
      "[83]\tvalid_0's multi_logloss: 0.724641\n",
      "[84]\tvalid_0's multi_logloss: 0.718158\n",
      "[85]\tvalid_0's multi_logloss: 0.71182\n",
      "[86]\tvalid_0's multi_logloss: 0.705539\n",
      "[87]\tvalid_0's multi_logloss: 0.699301\n",
      "[88]\tvalid_0's multi_logloss: 0.693311\n",
      "[89]\tvalid_0's multi_logloss: 0.687361\n",
      "[90]\tvalid_0's multi_logloss: 0.681586\n",
      "[91]\tvalid_0's multi_logloss: 0.675867\n",
      "[92]\tvalid_0's multi_logloss: 0.670232\n",
      "[93]\tvalid_0's multi_logloss: 0.664805\n",
      "[94]\tvalid_0's multi_logloss: 0.659511\n",
      "[95]\tvalid_0's multi_logloss: 0.654218\n",
      "[96]\tvalid_0's multi_logloss: 0.648982\n",
      "[97]\tvalid_0's multi_logloss: 0.643908\n",
      "[98]\tvalid_0's multi_logloss: 0.638872\n",
      "[99]\tvalid_0's multi_logloss: 0.634006\n",
      "[100]\tvalid_0's multi_logloss: 0.629181\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.629181\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(lgb_params, train_ds, 100, test_ds, verbose_eval=True, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#joblib.dump(model, 'lgbmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('lgbmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 3.4526\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's multi_logloss: 3.19536\n",
      "[3]\tvalid_0's multi_logloss: 3.0072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-51464ff2f7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboosting_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gbdt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'multi_logloss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m777\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_Y_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_Y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logloss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    803\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    598\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1974\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1975\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1977\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model2.fit(lgb_params, train_ds, 100, test_ds, early_stopping_rounds=20)\n",
    "model3 = lgb.LGBMClassifier(learning_rate=0.02, boosting_type = 'gbdt', metric = 'multi_logloss', seed = 777, n_jobs = -1)\n",
    "evals = [(r_X_test, r_Y_test)]\n",
    "model3.fit(r_X_train, r_Y_train, eval_set=evals, eval_metric='logloss', early_stopping_rounds=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 2.33278\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's multi_logloss: 2.20712\n",
      "[3]\tvalid_0's multi_logloss: 2.12642\n",
      "[4]\tvalid_0's multi_logloss: 2.05767\n",
      "[5]\tvalid_0's multi_logloss: 1.98369\n",
      "[6]\tvalid_0's multi_logloss: 1.89083\n",
      "[7]\tvalid_0's multi_logloss: 1.82254\n",
      "[8]\tvalid_0's multi_logloss: 1.8309\n",
      "[9]\tvalid_0's multi_logloss: 1.77147\n",
      "[10]\tvalid_0's multi_logloss: 1.7149\n",
      "[11]\tvalid_0's multi_logloss: 1.75125\n",
      "[12]\tvalid_0's multi_logloss: 1.78819\n",
      "[13]\tvalid_0's multi_logloss: 1.72567\n",
      "[14]\tvalid_0's multi_logloss: 1.70733\n",
      "[15]\tvalid_0's multi_logloss: 1.71452\n",
      "[16]\tvalid_0's multi_logloss: 1.69621\n",
      "[17]\tvalid_0's multi_logloss: 1.73788\n",
      "[18]\tvalid_0's multi_logloss: 1.79414\n",
      "[19]\tvalid_0's multi_logloss: 1.78671\n",
      "[20]\tvalid_0's multi_logloss: 1.8138\n",
      "[21]\tvalid_0's multi_logloss: 1.84212\n",
      "[22]\tvalid_0's multi_logloss: 1.86583\n",
      "[23]\tvalid_0's multi_logloss: 1.80044\n",
      "[24]\tvalid_0's multi_logloss: 1.86708\n",
      "[25]\tvalid_0's multi_logloss: 1.8804\n",
      "[26]\tvalid_0's multi_logloss: 2.00093\n",
      "[27]\tvalid_0's multi_logloss: 2.05974\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-e675cd560d40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'multi_logloss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_Y_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_Y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logloss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    803\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    598\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1974\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1975\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1977\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model2.fit(lgb_params, train_ds, 100, test_ds, early_stopping_rounds=20)\n",
    "model3 = lgb.LGBMClassifier(learning_rate=0.05, metric = 'multi_logloss', seed = 777)\n",
    "evals = [(r_X_test, r_Y_test)]\n",
    "model3.fit(r_X_train, r_Y_train, eval_set=evals, eval_metric='logloss', early_stopping_rounds=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgbmodel_3.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model3, 'lgbmodel_rob.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.78      0.86        23\n",
      "           1       1.00      0.68      0.81        25\n",
      "           2       1.00      0.76      0.86        21\n",
      "           3       0.97      0.85      0.91        41\n",
      "           4       1.00      0.88      0.94        34\n",
      "           5       0.91      0.84      0.87        25\n",
      "           6       1.00      0.83      0.91        36\n",
      "           7       1.00      0.91      0.95        32\n",
      "           8       1.00      1.00      1.00       110\n",
      "           9       0.56      0.14      0.23        35\n",
      "          10       0.32      0.39      0.35        28\n",
      "          11       0.00      0.00      0.00        16\n",
      "          12       0.88      0.33      0.48        21\n",
      "          13       0.75      0.22      0.34        27\n",
      "          14       0.54      0.26      0.35        27\n",
      "          15       0.65      0.83      0.73        18\n",
      "          16       0.36      0.36      0.36        22\n",
      "          17       0.80      0.99      0.88       235\n",
      "          18       1.00      1.00      1.00       124\n",
      "          19       0.82      0.85      0.84        54\n",
      "          20       0.84      0.68      0.75        31\n",
      "          21       1.00      0.89      0.94        53\n",
      "          22       0.90      0.95      0.92        39\n",
      "          23       1.00      1.00      1.00        68\n",
      "          24       1.00      0.76      0.86        21\n",
      "          25       1.00      0.83      0.90        23\n",
      "          26       1.00      1.00      1.00        24\n",
      "          27       1.00      0.90      0.95        31\n",
      "          28       1.00      1.00      1.00       180\n",
      "          29       1.00      1.00      1.00        51\n",
      "          30       1.00      1.00      1.00        48\n",
      "          31       1.00      1.00      1.00        11\n",
      "          32       1.00      1.00      1.00        11\n",
      "          33       1.00      0.92      0.96        12\n",
      "          34       1.00      1.00      1.00        10\n",
      "          35       1.00      1.00      1.00        15\n",
      "          36       0.67      0.17      0.27        12\n",
      "          37       1.00      0.93      0.97        15\n",
      "          38       0.99      1.00      1.00       182\n",
      "          39       1.00      1.00      1.00         6\n",
      "          40       1.00      0.99      0.99        96\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        10\n",
      "          43       1.00      1.00      1.00         7\n",
      "          44       1.00      1.00      1.00         9\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00         7\n",
      "          47       1.00      1.00      1.00        10\n",
      "          48       1.00      1.00      1.00        13\n",
      "          49       1.00      1.00      1.00        10\n",
      "          50       1.00      1.00      1.00         9\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00         7\n",
      "          53       1.00      1.00      1.00        68\n",
      "          54       0.97      0.98      0.98       104\n",
      "          55       0.80      0.40      0.53        10\n",
      "          56       1.00      0.11      0.20         9\n",
      "          57       0.00      0.00      0.00         5\n",
      "          58       0.75      0.38      0.50         8\n",
      "          59       0.50      0.11      0.18         9\n",
      "          60       0.00      0.00      0.00        15\n",
      "          61       1.00      1.00      1.00        53\n",
      "          62       1.00      1.00      1.00        47\n",
      "          63       0.83      0.76      0.79        25\n",
      "          64       0.81      0.86      0.83        29\n",
      "          65       0.86      0.94      0.90        33\n",
      "          66       0.92      0.82      0.87        28\n",
      "          67       0.98      1.00      0.99        43\n",
      "          68       1.00      1.00      1.00       147\n",
      "          69       1.00      1.00      1.00       116\n",
      "          70       1.00      0.86      0.92        14\n",
      "          71       0.00      0.00      0.00        10\n",
      "          72       1.00      0.15      0.27        13\n",
      "          73       1.00      0.45      0.62        20\n",
      "          74       1.00      0.12      0.22         8\n",
      "          75       1.00      1.00      1.00        72\n",
      "          76       1.00      1.00      1.00        83\n",
      "          77       0.25      0.49      0.33        71\n",
      "          78       1.00      1.00      1.00        29\n",
      "          79       1.00      0.69      0.82        26\n",
      "          80       1.00      0.54      0.70        24\n",
      "          81       1.00      0.59      0.75        37\n",
      "          82       1.00      0.50      0.67        16\n",
      "          83       0.33      0.70      0.44       109\n",
      "          84       1.00      0.86      0.92        21\n",
      "          85       0.98      1.00      0.99        57\n",
      "          86       1.00      1.00      1.00        47\n",
      "          87       1.00      0.79      0.88        24\n",
      "          88       1.00      0.85      0.92        33\n",
      "          89       1.00      0.93      0.96        29\n",
      "          90       0.94      0.85      0.89        34\n",
      "          91       0.74      0.65      0.69       105\n",
      "          92       1.00      1.00      1.00         8\n",
      "          93       1.00      0.73      0.84        11\n",
      "          94       0.88      1.00      0.93         7\n",
      "          95       1.00      1.00      1.00        10\n",
      "          96       1.00      1.00      1.00        10\n",
      "          97       1.00      0.92      0.96        12\n",
      "          98       1.00      1.00      1.00        10\n",
      "          99       1.00      1.00      1.00        10\n",
      "         100       1.00      1.00      1.00        14\n",
      "         101       0.00      0.00      0.00         7\n",
      "         102       0.00      0.00      0.00        12\n",
      "         103       0.50      0.12      0.20         8\n",
      "         104       0.00      0.00      0.00         7\n",
      "         105       0.00      0.00      0.00         9\n",
      "         106       0.00      0.00      0.00        11\n",
      "         107       1.00      0.22      0.36         9\n",
      "         108       0.25      0.11      0.15         9\n",
      "         109       0.25      0.12      0.17         8\n",
      "         110       0.52      1.00      0.68       227\n",
      "         111       1.00      1.00      1.00       159\n",
      "         112       1.00      1.00      1.00       201\n",
      "         113       1.00      1.00      1.00       176\n",
      "         114       1.00      1.00      1.00       169\n",
      "         115       1.00      1.00      1.00       168\n",
      "         116       0.99      1.00      1.00       150\n",
      "         117       1.00      1.00      1.00       178\n",
      "         118       1.00      1.00      1.00       206\n",
      "         119       0.72      0.65      0.69        55\n",
      "         120       0.68      0.70      0.69        37\n",
      "         121       1.00      0.83      0.91        54\n",
      "         122       0.80      0.69      0.74        62\n",
      "         123       0.65      0.60      0.63        50\n",
      "         124       0.84      0.77      0.80        47\n",
      "         125       1.00      1.00      1.00        49\n",
      "         126       1.00      1.00      1.00       111\n",
      "         127       1.00      1.00      1.00        34\n",
      "         128       1.00      1.00      1.00       122\n",
      "         129       1.00      1.00      1.00        44\n",
      "         130       1.00      1.00      1.00        95\n",
      "         131       1.00      1.00      1.00        53\n",
      "         132       0.98      1.00      0.99       101\n",
      "         133       1.00      0.90      0.95        10\n",
      "         134       1.00      0.90      0.95        10\n",
      "         135       0.82      0.82      0.82        11\n",
      "         136       0.91      0.93      0.92        43\n",
      "         137       1.00      0.76      0.87        17\n",
      "         138       0.73      0.73      0.73        11\n",
      "         139       0.77      0.91      0.83        11\n",
      "         140       1.00      0.71      0.83        14\n",
      "         141       0.62      0.71      0.67         7\n",
      "         142       0.67      0.40      0.50        10\n",
      "         143       0.91      0.91      0.91        11\n",
      "         144       1.00      0.86      0.92         7\n",
      "         145       0.90      0.90      0.90        10\n",
      "         146       0.60      0.75      0.67         8\n",
      "         147       0.98      0.92      0.95        52\n",
      "         148       1.00      0.88      0.93         8\n",
      "         149       1.00      1.00      1.00        12\n",
      "         150       1.00      1.00      1.00        14\n",
      "         151       0.93      0.93      0.93        15\n",
      "         152       0.83      0.83      0.83         6\n",
      "         153       1.00      0.91      0.95        11\n",
      "         154       0.91      1.00      0.95        10\n",
      "         155       1.00      0.98      0.99        44\n",
      "         156       1.00      1.00      1.00        29\n",
      "         157       1.00      1.00      1.00        30\n",
      "         158       1.00      1.00      1.00        53\n",
      "         159       1.00      1.00      1.00        34\n",
      "         160       1.00      1.00      1.00        49\n",
      "         161       1.00      1.00      1.00         7\n",
      "         162       1.00      1.00      1.00        59\n",
      "         163       1.00      1.00      1.00        11\n",
      "         164       1.00      1.00      1.00        47\n",
      "         165       1.00      1.00      1.00        29\n",
      "         166       1.00      1.00      1.00        36\n",
      "         167       1.00      1.00      1.00        37\n",
      "         168       1.00      1.00      1.00        44\n",
      "         169       1.00      1.00      1.00        32\n",
      "         170       0.00      0.00      0.00         9\n",
      "         171       0.00      0.00      0.00         5\n",
      "         172       1.00      1.00      1.00        67\n",
      "         173       1.00      1.00      1.00        88\n",
      "         174       1.00      1.00      1.00        73\n",
      "         175       1.00      1.00      1.00        85\n",
      "         176       1.00      1.00      1.00       101\n",
      "         177       1.00      1.00      1.00        80\n",
      "         178       1.00      1.00      1.00        84\n",
      "         179       1.00      1.00      1.00        73\n",
      "         180       1.00      1.00      1.00        91\n",
      "         181       1.00      1.00      1.00        98\n",
      "         182       0.00      0.00      0.00        12\n",
      "         183       1.00      1.00      1.00        11\n",
      "         184       1.00      1.00      1.00        10\n",
      "         185       1.00      1.00      1.00        10\n",
      "         186       1.00      1.00      1.00         9\n",
      "         187       1.00      1.00      1.00        15\n",
      "         188       1.00      1.00      1.00        10\n",
      "         189       1.00      0.92      0.96        13\n",
      "         190       1.00      1.00      1.00        13\n",
      "         191       1.00      0.90      0.95        10\n",
      "         192       1.00      1.00      1.00        40\n",
      "         193       1.00      0.86      0.92        21\n",
      "         194       1.00      0.60      0.75        15\n",
      "         195       1.00      0.55      0.71        11\n",
      "         196       1.00      0.23      0.38        13\n",
      "         197       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.91      8280\n",
      "   macro avg       0.86      0.79      0.81      8280\n",
      "weighted avg       0.92      0.91      0.90      8280\n",
      "\n",
      "\n",
      "[[18  0  0 ...  0  0  0]\n",
      " [ 0 17  0 ...  0  0  0]\n",
      " [ 0  0 16 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  6  0  0]\n",
      " [ 0  0  0 ...  0  3  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#expected_y  = Y_test\n",
    "#predicted_y = model3.predict(X_test).round()\n",
    "#print(); print('LightGBM: ')\n",
    "#print(predicted_y)\n",
    "print(); print(metrics.classification_report(expected_y, predicted_y))\n",
    "print(); print(metrics.confusion_matrix(expected_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model3.predict_proba(test)\n",
    "submission = pd.DataFrame(data=pred)\n",
    "submission.index = test.index\n",
    "submission.index.name = 'id'\n",
    "submission = submission.sort_index()\n",
    "submission = submission.groupby('id').mean().round(6)\n",
    "submission.to_csv('submission_3.csv', index=True) #제출 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
