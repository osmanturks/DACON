{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt # 데이터 시각화\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import os\n",
    "import re\n",
    "import multiprocessing # 여러 개의 일꾼 (cpu)들에게 작업을 분산시키는 역할\n",
    "from multiprocessing import Pool \n",
    "from functools import partial # 함수가 받는 인자들 중 몇개를 고정 시켜서 새롭게 파생된 함수를 형성하는 역할\n",
    "from data_loader_v2 import data_loader_v2\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (13,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "import lightgbm as lgb\n",
    "seed = 777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_label = pd.read_csv(\"train_label.csv\")\n",
    "plt.hist(train_label['label'], bins=len(train_label['label'].unique()))\n",
    "plt.xlim(0,200)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('# of label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체적으로 Imbalance Data, 1개뿐인 라벨이 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad, CLOSE, Equip Fail, No Data, Normal, OFF, ON, OPEN, System.Char[] 등 문자열 데이터를 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "realData1 = \"Train/30.csv\"\n",
    "realData2 = \"Test/1154.csv\"\n",
    "realData3 = \"Test/1168.csv\"\n",
    "realData4 = \"additinal_data/additinal_data1\"\n",
    "realData5 = \"additinal_data/additinal_data2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'train/'\n",
    "test_folder = 'test/'\n",
    "train_label_path = 'train_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = os.listdir(train_folder)\n",
    "test_list = os.listdir(test_folder)\n",
    "train_label = pd.read_csv(train_label_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataPreprocessn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 csv 파일의 상태_B로 변화는 시점이 10초로 가정\n",
    "def data_loader_v2_all(func, files, folder='', train_label=None, event_time=10, nrows=60):   \n",
    "    func_fixed = partial(func, folder=folder, train_label=train_label, event_time=event_time, nrows=nrows)     \n",
    "    if __name__ == '__main__':\n",
    "        pool = Pool(processes=multiprocessing.cpu_count()) \n",
    "        df_list = list(pool.imap(func_fixed, files)) \n",
    "        pool.close()\n",
    "        pool.join()        \n",
    "    combined_df = pd.concat(df_list)    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = data_loader_v2_all(data_loader_v2, train_list, folder=train_folder, train_label=train_label, event_time=10, nrows=60)\n",
    "#train.to_csv(\"base_train.csv\")\n",
    "train = pd.read_csv(\"base_train.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 test 셋 데이터에 대해서 10초부터 상태_B가 시작된다고 가정 \n",
    "test = data_loader_v2_all(data_loader_v2, test_list, folder=test_folder, train_label=None, event_time=10, nrows=60)\n",
    "test.to_csv(\"base_test.csv\")\n",
    "test = pd.read_csv(\"base_test.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train.copy()\n",
    "train_y = train_1['label']\n",
    "train_1 = train_1.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0      V0000     V0001     V0002     V0003     V0004  \\\n",
      "25920         666  30.460902  8.696729  8.678150  8.707048  8.743941   \n",
      "31215         675  30.467751  8.634908  8.721089  8.677828  8.720551   \n",
      "6486          547  30.473759  8.729892  8.689503  8.666899  8.672289   \n",
      "25275         248  30.484401  8.759098  8.716958  8.730716  8.687358   \n",
      "32890         389  30.469542  8.785799  8.717737  8.735726  8.717749   \n",
      "...           ...        ...       ...       ...       ...       ...   \n",
      "10879         811  23.635265  4.311902  4.222889  3.817453  3.780163   \n",
      "26695         317  30.477458  8.750068  8.722127  8.723330  8.707557   \n",
      "36785         589  30.470703  8.831234  8.686700  8.706548  8.714215   \n",
      "40535         819  30.446290  8.736160  8.680907  8.678161  8.692547   \n",
      "15931         525  30.476703  8.689716  8.664188  8.677084  8.653015   \n",
      "\n",
      "            V0005       V0006         V0007  V0008  ...  V5111  V5112  V5113  \\\n",
      "25920  187.271493  220.187777 -2.609061e-19    0.0  ...    1.0    1.0    1.0   \n",
      "31215  208.165981  261.808273 -2.894820e-19    0.0  ...    1.0    1.0    1.0   \n",
      "6486   253.143294  227.703277  3.146973e-19    0.0  ...    1.0    1.0    1.0   \n",
      "25275  178.700974  184.457616  4.747937e-19    0.0  ...    1.0    1.0    1.0   \n",
      "32890  176.454884  113.840214 -2.867486e-19    0.0  ...    1.0    1.0    1.0   \n",
      "...           ...         ...           ...    ...  ...    ...    ...    ...   \n",
      "10879  -23.022390   27.120105  2.651098e+02    0.0  ...    1.0    1.0    1.0   \n",
      "26695  196.777031  196.206859  7.336516e-19    0.0  ...    1.0    1.0    1.0   \n",
      "36785  187.066875  188.170848  4.909930e-20    0.0  ...    1.0    1.0    1.0   \n",
      "40535  279.684095  263.235703 -4.892852e-19    0.0  ...    1.0    1.0    1.0   \n",
      "15931  239.878768  265.317991  6.309064e-19    0.0  ...    1.0    1.0    1.0   \n",
      "\n",
      "       V5114  V5115  V5116  V5117      V5118     V5119  V5120  \n",
      "25920    1.0   60.0    0.0    0.0  -0.000003  85.40000    0.0  \n",
      "31215    1.0   60.0    0.0    0.0  -0.000003  85.40000    0.0  \n",
      "6486     1.0   60.0    0.0    0.0  -0.000012  85.40000    0.0  \n",
      "25275    1.0   60.0    0.0    0.0   0.000008  85.40000    0.0  \n",
      "32890    1.0   60.0    0.0    0.0   0.000020  85.40000    0.0  \n",
      "...      ...    ...    ...    ...        ...       ...    ...  \n",
      "10879    1.0    0.1    0.0    0.0  70.678238   8.82437    0.0  \n",
      "26695    1.0   60.0    0.0    0.0  -0.000009  85.40000    0.0  \n",
      "36785    1.0   60.0    0.0    0.0  -0.000002  85.40000    0.0  \n",
      "40535    1.0   60.0    0.0    0.0  -0.000004  85.40000    0.0  \n",
      "15931    1.0   60.0    0.0    0.0  -0.000015  85.40000    0.0  \n",
      "\n",
      "[33120 rows x 5122 columns]        Unnamed: 0      V0000     V0001     V0002     V0003     V0004  \\\n",
      "20639         290  30.466830  8.544912  8.474796  8.495731  8.463197   \n",
      "38743         588  30.475909  8.791712  8.705517  8.661790  8.690852   \n",
      "37998         359  30.489189  8.626852  8.545856  8.695577  8.689374   \n",
      "36720         204  30.480330  8.740064  8.721607  8.703408  8.719145   \n",
      "31138         107  30.485783  8.664525  8.710334  8.707884  8.714969   \n",
      "...           ...        ...       ...       ...       ...       ...   \n",
      "33252          60  30.456247  8.836413  8.716750  8.674596  8.705695   \n",
      "21872         319  30.465355  8.790987  8.689226  8.718441  8.735292   \n",
      "3438          220  30.483190  8.836461  8.704230  8.730588  8.711972   \n",
      "27225         707  30.473985  8.622354  8.683352  8.689707  8.698356   \n",
      "26319         302  30.488181  8.581986  8.478567  8.476242  8.520133   \n",
      "\n",
      "            V0005       V0006         V0007      V0008  ...  V5111  V5112  \\\n",
      "20639  268.175079  276.259286  2.613470e+02  261.00864  ...    1.0    1.0   \n",
      "38743  240.634524  197.651943  1.634715e-20    0.00000  ...    1.0    1.0   \n",
      "37998  213.208885  220.488462  5.043719e-19    0.00000  ...    1.0    1.0   \n",
      "36720  155.095346  168.565382  4.265669e-19    0.00000  ...    1.0    1.0   \n",
      "31138  197.504720  160.205515 -4.186459e-20    0.00000  ...    1.0    1.0   \n",
      "...           ...         ...           ...        ...  ...    ...    ...   \n",
      "33252  173.506788  200.769245  4.099004e-19    0.00000  ...    1.0    1.0   \n",
      "21872  165.127928  176.782001  1.694838e-19    0.00000  ...    1.0    1.0   \n",
      "3438   190.621019  211.138741  3.666355e-19    0.00000  ...    1.0    1.0   \n",
      "27225  178.334101  151.790755 -4.695022e-19    0.00000  ...    1.0    1.0   \n",
      "26319  261.774746  279.461111  4.948041e-19    0.00000  ...    1.0    1.0   \n",
      "\n",
      "       V5113  V5114    V5115  V5116  V5117         V5118     V5119  V5120  \n",
      "20639    1.0    1.0  60.0000    0.0    0.0  5.866493e-06  85.40000    0.0  \n",
      "38743    1.0    1.0  60.0000    0.0    0.0  7.471110e-06  85.40000    0.0  \n",
      "37998    1.0    1.0  60.0000    0.0    0.0 -7.269697e-07  85.40000    0.0  \n",
      "36720    1.0    1.0  60.0000    0.0    0.0 -8.129589e-06  85.40000    0.0  \n",
      "31138    1.0    1.0  60.0000    0.0    0.0 -8.963084e-06  85.40000    0.0  \n",
      "...      ...    ...      ...    ...    ...           ...       ...    ...  \n",
      "33252    1.0    1.0  60.0000    0.0    0.0  9.646324e-06  85.40000    0.0  \n",
      "21872    1.0    1.0  60.0000    0.0    0.0 -2.462708e-05  85.40000    0.0  \n",
      "3438     1.0    1.0  60.0000    0.0    0.0 -1.072581e-05  85.40000    0.0  \n",
      "27225    1.0    1.0  60.0000    0.0    0.0 -2.109555e-06  85.40000    0.0  \n",
      "26319    1.0    1.0  59.8849    0.0    0.0  1.083335e-05   6.12653    0.0  \n",
      "\n",
      "[8280 rows x 5122 columns] 25920     74\n",
      "31215    112\n",
      "6486     155\n",
      "25275    195\n",
      "32890    148\n",
      "        ... \n",
      "10879     53\n",
      "26695    114\n",
      "36785    179\n",
      "40535     38\n",
      "15931     38\n",
      "Name: label, Length: 33120, dtype: int64 20639      0\n",
      "38743    156\n",
      "37998     25\n",
      "36720    169\n",
      "31138    111\n",
      "        ... \n",
      "33252    172\n",
      "21872    122\n",
      "3438     119\n",
      "27225     68\n",
      "26319     49\n",
      "Name: label, Length: 8280, dtype: int64\n",
      "(33120, 5122) (8280, 5122) (33120,) (8280,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(train_1, train_y, test_size=0.2, random_state=seed)\n",
    "print(X_train,X_test,Y_train,Y_test)\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_train = stdScaler.fit_transform(X_train)\n",
    "X_Test = stdScaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClaasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=-1, oob_score=False, random_state=0, verbose=1,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=0, verbose=1, n_jobs=-1)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        20\n",
      "           1       1.00      0.47      0.64        17\n",
      "           2       0.95      0.82      0.88        22\n",
      "           3       0.91      0.80      0.85        25\n",
      "           4       0.96      0.71      0.82        38\n",
      "           5       1.00      0.85      0.92        41\n",
      "           6       1.00      0.92      0.96        37\n",
      "           7       1.00      0.93      0.96        28\n",
      "           8       0.68      0.92      0.78       125\n",
      "           9       0.23      0.11      0.15        28\n",
      "          10       0.33      0.75      0.46        32\n",
      "          11       0.00      0.00      0.00        22\n",
      "          12       0.88      0.28      0.42        25\n",
      "          13       1.00      0.06      0.11        35\n",
      "          14       0.25      0.27      0.26        30\n",
      "          15       0.60      0.56      0.58        16\n",
      "          16       0.40      0.29      0.33        35\n",
      "          17       0.72      0.95      0.82       216\n",
      "          18       0.99      0.97      0.98       105\n",
      "          19       0.80      0.84      0.82        51\n",
      "          20       0.71      0.43      0.54        23\n",
      "          21       0.98      1.00      0.99        56\n",
      "          22       1.00      0.97      0.99        40\n",
      "          23       0.85      0.88      0.86        76\n",
      "          24       1.00      0.86      0.93        22\n",
      "          25       1.00      0.83      0.91        30\n",
      "          26       1.00      0.93      0.96        27\n",
      "          27       1.00      0.89      0.94        28\n",
      "          28       0.88      0.93      0.91       180\n",
      "          29       0.94      0.89      0.92        57\n",
      "          30       1.00      0.96      0.98        56\n",
      "          31       1.00      1.00      1.00         7\n",
      "          32       0.86      0.86      0.86         7\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.93      0.88      0.90        16\n",
      "          35       1.00      0.92      0.96        12\n",
      "          36       1.00      0.27      0.43        11\n",
      "          37       1.00      0.67      0.80        12\n",
      "          38       0.85      0.96      0.90       183\n",
      "          39       1.00      0.89      0.94         9\n",
      "          40       0.98      0.96      0.97        85\n",
      "          41       0.92      1.00      0.96        11\n",
      "          42       0.86      0.75      0.80         8\n",
      "          43       0.83      0.91      0.87        11\n",
      "          44       1.00      0.81      0.90        16\n",
      "          45       0.89      0.89      0.89         9\n",
      "          46       0.88      0.88      0.88         8\n",
      "          47       1.00      0.91      0.95        11\n",
      "          48       0.64      0.78      0.70         9\n",
      "          49       0.83      0.91      0.87        11\n",
      "          50       1.00      0.67      0.80         6\n",
      "          51       0.90      1.00      0.95         9\n",
      "          52       0.73      0.89      0.80         9\n",
      "          53       0.98      0.98      0.98        54\n",
      "          54       0.98      0.93      0.95       112\n",
      "          55       0.80      0.40      0.53        10\n",
      "          56       0.25      0.08      0.12        12\n",
      "          57       0.33      0.08      0.12        13\n",
      "          58       1.00      0.73      0.85        15\n",
      "          59       0.50      0.18      0.27        11\n",
      "          60       0.62      0.50      0.56        10\n",
      "          61       1.00      1.00      1.00        43\n",
      "          62       1.00      0.98      0.99        55\n",
      "          63       0.67      0.58      0.62        31\n",
      "          64       0.69      0.66      0.68        38\n",
      "          65       0.88      0.50      0.64        42\n",
      "          66       0.56      0.96      0.71        23\n",
      "          67       0.47      0.82      0.60        38\n",
      "          68       1.00      0.98      0.99       131\n",
      "          69       0.91      0.92      0.92       157\n",
      "          70       1.00      0.77      0.87        13\n",
      "          71       0.00      0.00      0.00        10\n",
      "          72       0.75      0.50      0.60         6\n",
      "          73       0.90      0.50      0.64        18\n",
      "          74       0.75      0.30      0.43        10\n",
      "          75       0.95      0.89      0.92        83\n",
      "          76       0.87      0.93      0.90        80\n",
      "          77       0.29      0.32      0.31        87\n",
      "          78       0.97      0.90      0.94        40\n",
      "          79       0.81      0.63      0.71        35\n",
      "          80       0.88      0.27      0.41        26\n",
      "          81       1.00      0.52      0.68        27\n",
      "          82       1.00      0.60      0.75        15\n",
      "          83       0.32      0.79      0.46       105\n",
      "          84       1.00      0.95      0.98        22\n",
      "          85       0.92      0.92      0.92        73\n",
      "          86       1.00      0.97      0.98        33\n",
      "          87       1.00      0.88      0.94        25\n",
      "          88       1.00      0.75      0.86        28\n",
      "          89       1.00      0.90      0.95        20\n",
      "          90       1.00      0.85      0.92        26\n",
      "          91       0.68      0.70      0.69       105\n",
      "          92       1.00      0.93      0.96        14\n",
      "          93       1.00      0.82      0.90        11\n",
      "          94       1.00      0.80      0.89        10\n",
      "          95       1.00      1.00      1.00        11\n",
      "          96       1.00      1.00      1.00         8\n",
      "          97       1.00      1.00      1.00         6\n",
      "          98       1.00      0.75      0.86        12\n",
      "          99       1.00      1.00      1.00         6\n",
      "         100       0.92      0.92      0.92        12\n",
      "         101       1.00      0.22      0.36         9\n",
      "         102       0.00      0.00      0.00        10\n",
      "         103       0.00      0.00      0.00        18\n",
      "         104       0.60      0.33      0.43         9\n",
      "         105       0.67      0.14      0.24        14\n",
      "         106       1.00      0.11      0.20         9\n",
      "         107       1.00      0.70      0.82        10\n",
      "         108       0.00      0.00      0.00        12\n",
      "         109       1.00      0.33      0.50        12\n",
      "         110       0.79      0.95      0.86       242\n",
      "         111       0.85      0.93      0.89       192\n",
      "         112       0.92      0.95      0.93       192\n",
      "         113       0.97      0.94      0.95       201\n",
      "         114       0.80      0.94      0.87       206\n",
      "         115       0.98      0.97      0.98       176\n",
      "         116       1.00      0.98      0.99       181\n",
      "         117       1.00      0.95      0.97       182\n",
      "         118       0.86      0.93      0.90       235\n",
      "         119       0.63      0.56      0.59        59\n",
      "         120       0.65      0.59      0.62        44\n",
      "         121       0.84      0.80      0.82        45\n",
      "         122       0.65      0.59      0.62        69\n",
      "         123       0.53      0.57      0.55        61\n",
      "         124       0.90      0.70      0.79        61\n",
      "         125       0.98      0.93      0.95        55\n",
      "         126       0.93      0.95      0.94        93\n",
      "         127       0.95      0.97      0.96        39\n",
      "         128       0.86      0.93      0.89       117\n",
      "         129       0.94      0.92      0.93        49\n",
      "         130       0.90      0.84      0.87       115\n",
      "         131       0.91      0.93      0.92        42\n",
      "         132       0.74      0.97      0.83        89\n",
      "         133       1.00      0.62      0.77         8\n",
      "         134       1.00      0.92      0.96        12\n",
      "         135       1.00      0.78      0.88         9\n",
      "         136       0.97      0.83      0.89        46\n",
      "         137       1.00      0.83      0.91        12\n",
      "         138       1.00      0.91      0.95        11\n",
      "         139       1.00      0.73      0.85        15\n",
      "         140       1.00      0.58      0.74        12\n",
      "         141       1.00      0.91      0.95        11\n",
      "         142       0.60      0.50      0.55         6\n",
      "         143       1.00      0.64      0.78        14\n",
      "         144       1.00      0.46      0.63        13\n",
      "         145       0.88      0.58      0.70        12\n",
      "         146       1.00      0.83      0.91        12\n",
      "         147       0.85      0.83      0.84        47\n",
      "         148       1.00      0.58      0.74        12\n",
      "         149       0.80      1.00      0.89        12\n",
      "         150       1.00      0.50      0.67        12\n",
      "         151       1.00      0.70      0.82        10\n",
      "         152       1.00      0.64      0.78        11\n",
      "         153       1.00      0.67      0.80         9\n",
      "         154       1.00      0.70      0.82        10\n",
      "         155       0.87      0.89      0.88        38\n",
      "         156       0.92      0.90      0.91        39\n",
      "         157       0.94      0.88      0.91        33\n",
      "         158       0.85      0.86      0.85        58\n",
      "         159       1.00      0.82      0.90        17\n",
      "         160       0.94      0.82      0.87        55\n",
      "         161       1.00      0.73      0.85        15\n",
      "         162       0.41      0.92      0.57        60\n",
      "         163       1.00      1.00      1.00        10\n",
      "         164       0.94      0.89      0.91        53\n",
      "         165       0.90      0.66      0.76        41\n",
      "         166       0.78      0.84      0.81        45\n",
      "         167       0.95      0.97      0.96        40\n",
      "         168       0.46      0.82      0.59        50\n",
      "         169       0.65      0.56      0.60        43\n",
      "         170       0.00      0.00      0.00        10\n",
      "         171       0.40      0.25      0.31         8\n",
      "         172       0.93      0.92      0.93        77\n",
      "         173       0.98      0.91      0.94        96\n",
      "         174       0.99      0.89      0.94        93\n",
      "         175       1.00      0.96      0.98        77\n",
      "         176       0.98      0.90      0.94        93\n",
      "         177       0.95      0.91      0.93        89\n",
      "         178       1.00      0.98      0.99        83\n",
      "         179       1.00      0.99      0.99        79\n",
      "         180       1.00      0.92      0.96        83\n",
      "         181       0.93      0.93      0.93        83\n",
      "         182       0.00      0.00      0.00        12\n",
      "         183       1.00      1.00      1.00        11\n",
      "         184       0.92      1.00      0.96        11\n",
      "         185       1.00      0.75      0.86         4\n",
      "         186       1.00      1.00      1.00         5\n",
      "         187       1.00      0.85      0.92        13\n",
      "         188       1.00      1.00      1.00        11\n",
      "         189       1.00      1.00      1.00         7\n",
      "         190       1.00      0.67      0.80        15\n",
      "         191       1.00      1.00      1.00        10\n",
      "         192       0.91      0.91      0.91        23\n",
      "         193       1.00      0.74      0.85        23\n",
      "         194       1.00      0.62      0.76        13\n",
      "         195       1.00      0.50      0.67        10\n",
      "         196       0.45      0.71      0.56         7\n",
      "         197       1.00      0.07      0.12        15\n",
      "\n",
      "    accuracy                           0.84      8602\n",
      "   macro avg       0.85      0.74      0.77      8602\n",
      "weighted avg       0.86      0.84      0.84      8602\n",
      "\n",
      "\n",
      "[[16  0  0 ...  0  0  0]\n",
      " [ 0  8  1 ...  0  0  0]\n",
      " [ 0  0 18 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  5  1  0]\n",
      " [ 0  0  0 ...  0  5  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "expected_y  = Y_test\n",
    "predicted_y = model.predict(X_test).round()\n",
    "print(); print('RandomForestClassifier: ')\n",
    "print(); print(metrics.classification_report(expected_y, predicted_y))\n",
    "print(); print(metrics.confusion_matrix(expected_y, predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-c882ced9a0e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "train_1.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train_1.columns]\n",
    "\n",
    "X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "X_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n",
    "\n",
    "train_ds = lgb.Dataset(X_train, label = Y_train)\n",
    "test_ds = lgb.Dataset(X_test, label = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "#model.fit(X_train,Y_train)\n",
    "lgb_params = {\n",
    "        'objective':'multiclass',\n",
    "        'metric':'multi_logloss',\n",
    "        'num_class':198,\n",
    "        'learning_rate':0.02,\n",
    "        'seed': 5,\n",
    "        'n_jobs': -1\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 3.28624\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's multi_logloss: 3.02749\n",
      "[3]\tvalid_0's multi_logloss: 2.84508\n",
      "[4]\tvalid_0's multi_logloss: 2.69981\n",
      "[5]\tvalid_0's multi_logloss: 2.57696\n",
      "[6]\tvalid_0's multi_logloss: 2.47148\n",
      "[7]\tvalid_0's multi_logloss: 2.37878\n",
      "[8]\tvalid_0's multi_logloss: 2.29634\n",
      "[9]\tvalid_0's multi_logloss: 2.22167\n",
      "[10]\tvalid_0's multi_logloss: 2.15358\n",
      "[11]\tvalid_0's multi_logloss: 2.09138\n",
      "[12]\tvalid_0's multi_logloss: 2.03358\n",
      "[13]\tvalid_0's multi_logloss: 1.97991\n",
      "[14]\tvalid_0's multi_logloss: 1.92964\n",
      "[15]\tvalid_0's multi_logloss: 1.88244\n",
      "[16]\tvalid_0's multi_logloss: 1.83787\n",
      "[17]\tvalid_0's multi_logloss: 1.79593\n",
      "[18]\tvalid_0's multi_logloss: 1.75591\n",
      "[19]\tvalid_0's multi_logloss: 1.718\n",
      "[20]\tvalid_0's multi_logloss: 1.68209\n",
      "[21]\tvalid_0's multi_logloss: 1.64778\n",
      "[22]\tvalid_0's multi_logloss: 1.61461\n",
      "[23]\tvalid_0's multi_logloss: 1.58296\n",
      "[24]\tvalid_0's multi_logloss: 1.55287\n",
      "[25]\tvalid_0's multi_logloss: 1.5239\n",
      "[26]\tvalid_0's multi_logloss: 1.49611\n",
      "[27]\tvalid_0's multi_logloss: 1.46939\n",
      "[28]\tvalid_0's multi_logloss: 1.4435\n",
      "[29]\tvalid_0's multi_logloss: 1.41856\n",
      "[30]\tvalid_0's multi_logloss: 1.39445\n",
      "[31]\tvalid_0's multi_logloss: 1.37117\n",
      "[32]\tvalid_0's multi_logloss: 1.34856\n",
      "[33]\tvalid_0's multi_logloss: 1.32662\n",
      "[34]\tvalid_0's multi_logloss: 1.30536\n",
      "[35]\tvalid_0's multi_logloss: 1.28497\n",
      "[36]\tvalid_0's multi_logloss: 1.26503\n",
      "[37]\tvalid_0's multi_logloss: 1.24566\n",
      "[38]\tvalid_0's multi_logloss: 1.22686\n",
      "[39]\tvalid_0's multi_logloss: 1.20864\n",
      "[40]\tvalid_0's multi_logloss: 1.19093\n",
      "[41]\tvalid_0's multi_logloss: 1.17367\n",
      "[42]\tvalid_0's multi_logloss: 1.15688\n",
      "[43]\tvalid_0's multi_logloss: 1.14054\n",
      "[44]\tvalid_0's multi_logloss: 1.12466\n",
      "[45]\tvalid_0's multi_logloss: 1.10934\n",
      "[46]\tvalid_0's multi_logloss: 1.0943\n",
      "[47]\tvalid_0's multi_logloss: 1.07961\n",
      "[48]\tvalid_0's multi_logloss: 1.06532\n",
      "[49]\tvalid_0's multi_logloss: 1.0513\n",
      "[50]\tvalid_0's multi_logloss: 1.03776\n",
      "[51]\tvalid_0's multi_logloss: 1.02442\n",
      "[52]\tvalid_0's multi_logloss: 1.0114\n",
      "[53]\tvalid_0's multi_logloss: 0.998729\n",
      "[54]\tvalid_0's multi_logloss: 0.986405\n",
      "[55]\tvalid_0's multi_logloss: 0.974285\n",
      "[56]\tvalid_0's multi_logloss: 0.962444\n",
      "[57]\tvalid_0's multi_logloss: 0.950977\n",
      "[58]\tvalid_0's multi_logloss: 0.93965\n",
      "[59]\tvalid_0's multi_logloss: 0.928659\n",
      "[60]\tvalid_0's multi_logloss: 0.917885\n",
      "[61]\tvalid_0's multi_logloss: 0.907379\n",
      "[62]\tvalid_0's multi_logloss: 0.89703\n",
      "[63]\tvalid_0's multi_logloss: 0.886962\n",
      "[64]\tvalid_0's multi_logloss: 0.877121\n",
      "[65]\tvalid_0's multi_logloss: 0.867564\n",
      "[66]\tvalid_0's multi_logloss: 0.858152\n",
      "[67]\tvalid_0's multi_logloss: 0.848929\n",
      "[68]\tvalid_0's multi_logloss: 0.839946\n",
      "[69]\tvalid_0's multi_logloss: 0.831047\n",
      "[70]\tvalid_0's multi_logloss: 0.82244\n",
      "[71]\tvalid_0's multi_logloss: 0.813973\n",
      "[72]\tvalid_0's multi_logloss: 0.80574\n",
      "[73]\tvalid_0's multi_logloss: 0.797593\n",
      "[74]\tvalid_0's multi_logloss: 0.789541\n",
      "[75]\tvalid_0's multi_logloss: 0.781708\n",
      "[76]\tvalid_0's multi_logloss: 0.774058\n",
      "[77]\tvalid_0's multi_logloss: 0.766586\n",
      "[78]\tvalid_0's multi_logloss: 0.759244\n",
      "[79]\tvalid_0's multi_logloss: 0.752037\n",
      "[80]\tvalid_0's multi_logloss: 0.744956\n",
      "[81]\tvalid_0's multi_logloss: 0.73802\n",
      "[82]\tvalid_0's multi_logloss: 0.731242\n",
      "[83]\tvalid_0's multi_logloss: 0.724641\n",
      "[84]\tvalid_0's multi_logloss: 0.718158\n",
      "[85]\tvalid_0's multi_logloss: 0.71182\n",
      "[86]\tvalid_0's multi_logloss: 0.705539\n",
      "[87]\tvalid_0's multi_logloss: 0.699301\n",
      "[88]\tvalid_0's multi_logloss: 0.693311\n",
      "[89]\tvalid_0's multi_logloss: 0.687361\n",
      "[90]\tvalid_0's multi_logloss: 0.681586\n",
      "[91]\tvalid_0's multi_logloss: 0.675867\n",
      "[92]\tvalid_0's multi_logloss: 0.670232\n",
      "[93]\tvalid_0's multi_logloss: 0.664805\n",
      "[94]\tvalid_0's multi_logloss: 0.659511\n",
      "[95]\tvalid_0's multi_logloss: 0.654218\n",
      "[96]\tvalid_0's multi_logloss: 0.648982\n",
      "[97]\tvalid_0's multi_logloss: 0.643908\n",
      "[98]\tvalid_0's multi_logloss: 0.638872\n",
      "[99]\tvalid_0's multi_logloss: 0.634006\n",
      "[100]\tvalid_0's multi_logloss: 0.629181\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.629181\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(lgb_params, train_ds, 100, test_ds, verbose_eval=True, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#joblib.dump(model, 'lgbmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('lgbmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 3.42254\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's multi_logloss: 3.16563\n",
      "[3]\tvalid_0's multi_logloss: 2.97826\n",
      "[4]\tvalid_0's multi_logloss: 2.82815\n",
      "[5]\tvalid_0's multi_logloss: 2.70191\n",
      "[6]\tvalid_0's multi_logloss: 2.59368\n",
      "[7]\tvalid_0's multi_logloss: 2.49889\n",
      "[8]\tvalid_0's multi_logloss: 2.41394\n",
      "[9]\tvalid_0's multi_logloss: 2.33726\n",
      "[10]\tvalid_0's multi_logloss: 2.26742\n",
      "[11]\tvalid_0's multi_logloss: 2.20302\n",
      "[12]\tvalid_0's multi_logloss: 2.14349\n",
      "[13]\tvalid_0's multi_logloss: 2.08791\n",
      "[14]\tvalid_0's multi_logloss: 2.03615\n",
      "[15]\tvalid_0's multi_logloss: 1.98765\n",
      "[16]\tvalid_0's multi_logloss: 1.94191\n",
      "[17]\tvalid_0's multi_logloss: 1.89833\n",
      "[18]\tvalid_0's multi_logloss: 1.85715\n",
      "[19]\tvalid_0's multi_logloss: 1.81802\n",
      "[20]\tvalid_0's multi_logloss: 1.78094\n",
      "[21]\tvalid_0's multi_logloss: 1.74565\n",
      "[22]\tvalid_0's multi_logloss: 1.71172\n",
      "[23]\tvalid_0's multi_logloss: 1.67943\n",
      "[24]\tvalid_0's multi_logloss: 1.64842\n",
      "[25]\tvalid_0's multi_logloss: 1.61867\n",
      "[26]\tvalid_0's multi_logloss: 1.58997\n",
      "[27]\tvalid_0's multi_logloss: 1.56225\n",
      "[28]\tvalid_0's multi_logloss: 1.5356\n",
      "[29]\tvalid_0's multi_logloss: 1.50992\n",
      "[30]\tvalid_0's multi_logloss: 1.48509\n",
      "[31]\tvalid_0's multi_logloss: 1.46102\n",
      "[32]\tvalid_0's multi_logloss: 1.4378\n",
      "[33]\tvalid_0's multi_logloss: 1.41535\n",
      "[34]\tvalid_0's multi_logloss: 1.39347\n",
      "[35]\tvalid_0's multi_logloss: 1.37241\n",
      "[36]\tvalid_0's multi_logloss: 1.35193\n",
      "[37]\tvalid_0's multi_logloss: 1.332\n",
      "[38]\tvalid_0's multi_logloss: 1.31267\n",
      "[39]\tvalid_0's multi_logloss: 1.29391\n",
      "[40]\tvalid_0's multi_logloss: 1.27567\n",
      "[41]\tvalid_0's multi_logloss: 1.25803\n",
      "[42]\tvalid_0's multi_logloss: 1.24083\n",
      "[43]\tvalid_0's multi_logloss: 1.22404\n",
      "[44]\tvalid_0's multi_logloss: 1.2077\n",
      "[45]\tvalid_0's multi_logloss: 1.1919\n",
      "[46]\tvalid_0's multi_logloss: 1.17651\n",
      "[47]\tvalid_0's multi_logloss: 1.16143\n",
      "[48]\tvalid_0's multi_logloss: 1.14677\n",
      "[49]\tvalid_0's multi_logloss: 1.1325\n",
      "[50]\tvalid_0's multi_logloss: 1.11847\n",
      "[51]\tvalid_0's multi_logloss: 1.10493\n",
      "[52]\tvalid_0's multi_logloss: 1.0916\n",
      "[53]\tvalid_0's multi_logloss: 1.07864\n",
      "[54]\tvalid_0's multi_logloss: 1.06605\n",
      "[55]\tvalid_0's multi_logloss: 1.05374\n",
      "[56]\tvalid_0's multi_logloss: 1.04172\n",
      "[57]\tvalid_0's multi_logloss: 1.02992\n",
      "[58]\tvalid_0's multi_logloss: 1.01842\n",
      "[59]\tvalid_0's multi_logloss: 1.00706\n",
      "[60]\tvalid_0's multi_logloss: 0.996025\n",
      "[61]\tvalid_0's multi_logloss: 0.985233\n",
      "[62]\tvalid_0's multi_logloss: 0.974688\n",
      "[63]\tvalid_0's multi_logloss: 0.964376\n",
      "[64]\tvalid_0's multi_logloss: 0.954165\n",
      "[65]\tvalid_0's multi_logloss: 0.944241\n",
      "[66]\tvalid_0's multi_logloss: 0.934584\n",
      "[67]\tvalid_0's multi_logloss: 0.925063\n",
      "[68]\tvalid_0's multi_logloss: 0.915781\n",
      "[69]\tvalid_0's multi_logloss: 0.906749\n",
      "[70]\tvalid_0's multi_logloss: 0.89788\n",
      "[71]\tvalid_0's multi_logloss: 0.889217\n",
      "[72]\tvalid_0's multi_logloss: 0.880731\n",
      "[73]\tvalid_0's multi_logloss: 0.87236\n",
      "[74]\tvalid_0's multi_logloss: 0.864192\n",
      "[75]\tvalid_0's multi_logloss: 0.856179\n",
      "[76]\tvalid_0's multi_logloss: 0.84832\n",
      "[77]\tvalid_0's multi_logloss: 0.840676\n",
      "[78]\tvalid_0's multi_logloss: 0.833152\n",
      "[79]\tvalid_0's multi_logloss: 0.82577\n",
      "[80]\tvalid_0's multi_logloss: 0.818576\n",
      "[81]\tvalid_0's multi_logloss: 0.811469\n",
      "[82]\tvalid_0's multi_logloss: 0.804571\n",
      "[83]\tvalid_0's multi_logloss: 0.797738\n",
      "[84]\tvalid_0's multi_logloss: 0.791064\n",
      "[85]\tvalid_0's multi_logloss: 0.784552\n",
      "[86]\tvalid_0's multi_logloss: 0.778127\n",
      "[87]\tvalid_0's multi_logloss: 0.771842\n",
      "[88]\tvalid_0's multi_logloss: 0.765692\n",
      "[89]\tvalid_0's multi_logloss: 0.759647\n",
      "[90]\tvalid_0's multi_logloss: 0.753696\n",
      "[91]\tvalid_0's multi_logloss: 0.747883\n",
      "[92]\tvalid_0's multi_logloss: 0.742171\n",
      "[93]\tvalid_0's multi_logloss: 0.736573\n",
      "[94]\tvalid_0's multi_logloss: 0.731065\n",
      "[95]\tvalid_0's multi_logloss: 0.72564\n",
      "[96]\tvalid_0's multi_logloss: 0.720385\n",
      "[97]\tvalid_0's multi_logloss: 0.715226\n",
      "[98]\tvalid_0's multi_logloss: 0.71014\n",
      "[99]\tvalid_0's multi_logloss: 0.705167\n",
      "[100]\tvalid_0's multi_logloss: 0.700263\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.700263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.02, max_depth=-1,\n",
       "               metric='multi_logloss', min_child_samples=20,\n",
       "               min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "               n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "               reg_alpha=0.0, reg_lambda=0.0, seed=777, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model2.fit(lgb_params, train_ds, 100, test_ds, early_stopping_rounds=20)\n",
    "model3 = lgb.LGBMClassifier(learning_rate=0.02, boosting_type = 'gbdt', metric = 'multi_logloss', seed = 777, n_jobs = -1)\n",
    "evals = [(X_test, Y_test)]\n",
    "model3.fit(X_train, Y_train, eval_set=evals, eval_metric='logloss', early_stopping_rounds=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgbmodel_3.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model3, 'lgbmodel_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.78      0.86        23\n",
      "           1       1.00      0.68      0.81        25\n",
      "           2       1.00      0.76      0.86        21\n",
      "           3       0.97      0.85      0.91        41\n",
      "           4       1.00      0.88      0.94        34\n",
      "           5       0.91      0.84      0.87        25\n",
      "           6       1.00      0.83      0.91        36\n",
      "           7       1.00      0.91      0.95        32\n",
      "           8       1.00      1.00      1.00       110\n",
      "           9       0.56      0.14      0.23        35\n",
      "          10       0.32      0.39      0.35        28\n",
      "          11       0.00      0.00      0.00        16\n",
      "          12       0.88      0.33      0.48        21\n",
      "          13       0.75      0.22      0.34        27\n",
      "          14       0.54      0.26      0.35        27\n",
      "          15       0.65      0.83      0.73        18\n",
      "          16       0.36      0.36      0.36        22\n",
      "          17       0.80      0.99      0.88       235\n",
      "          18       1.00      1.00      1.00       124\n",
      "          19       0.82      0.85      0.84        54\n",
      "          20       0.84      0.68      0.75        31\n",
      "          21       1.00      0.89      0.94        53\n",
      "          22       0.90      0.95      0.92        39\n",
      "          23       1.00      1.00      1.00        68\n",
      "          24       1.00      0.76      0.86        21\n",
      "          25       1.00      0.83      0.90        23\n",
      "          26       1.00      1.00      1.00        24\n",
      "          27       1.00      0.90      0.95        31\n",
      "          28       1.00      1.00      1.00       180\n",
      "          29       1.00      1.00      1.00        51\n",
      "          30       1.00      1.00      1.00        48\n",
      "          31       1.00      1.00      1.00        11\n",
      "          32       1.00      1.00      1.00        11\n",
      "          33       1.00      0.92      0.96        12\n",
      "          34       1.00      1.00      1.00        10\n",
      "          35       1.00      1.00      1.00        15\n",
      "          36       0.67      0.17      0.27        12\n",
      "          37       1.00      0.93      0.97        15\n",
      "          38       0.99      1.00      1.00       182\n",
      "          39       1.00      1.00      1.00         6\n",
      "          40       1.00      0.99      0.99        96\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        10\n",
      "          43       1.00      1.00      1.00         7\n",
      "          44       1.00      1.00      1.00         9\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00         7\n",
      "          47       1.00      1.00      1.00        10\n",
      "          48       1.00      1.00      1.00        13\n",
      "          49       1.00      1.00      1.00        10\n",
      "          50       1.00      1.00      1.00         9\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00         7\n",
      "          53       1.00      1.00      1.00        68\n",
      "          54       0.97      0.98      0.98       104\n",
      "          55       0.80      0.40      0.53        10\n",
      "          56       1.00      0.11      0.20         9\n",
      "          57       0.00      0.00      0.00         5\n",
      "          58       0.75      0.38      0.50         8\n",
      "          59       0.50      0.11      0.18         9\n",
      "          60       0.00      0.00      0.00        15\n",
      "          61       1.00      1.00      1.00        53\n",
      "          62       1.00      1.00      1.00        47\n",
      "          63       0.83      0.76      0.79        25\n",
      "          64       0.81      0.86      0.83        29\n",
      "          65       0.86      0.94      0.90        33\n",
      "          66       0.92      0.82      0.87        28\n",
      "          67       0.98      1.00      0.99        43\n",
      "          68       1.00      1.00      1.00       147\n",
      "          69       1.00      1.00      1.00       116\n",
      "          70       1.00      0.86      0.92        14\n",
      "          71       0.00      0.00      0.00        10\n",
      "          72       1.00      0.15      0.27        13\n",
      "          73       1.00      0.45      0.62        20\n",
      "          74       1.00      0.12      0.22         8\n",
      "          75       1.00      1.00      1.00        72\n",
      "          76       1.00      1.00      1.00        83\n",
      "          77       0.25      0.49      0.33        71\n",
      "          78       1.00      1.00      1.00        29\n",
      "          79       1.00      0.69      0.82        26\n",
      "          80       1.00      0.54      0.70        24\n",
      "          81       1.00      0.59      0.75        37\n",
      "          82       1.00      0.50      0.67        16\n",
      "          83       0.33      0.70      0.44       109\n",
      "          84       1.00      0.86      0.92        21\n",
      "          85       0.98      1.00      0.99        57\n",
      "          86       1.00      1.00      1.00        47\n",
      "          87       1.00      0.79      0.88        24\n",
      "          88       1.00      0.85      0.92        33\n",
      "          89       1.00      0.93      0.96        29\n",
      "          90       0.94      0.85      0.89        34\n",
      "          91       0.74      0.65      0.69       105\n",
      "          92       1.00      1.00      1.00         8\n",
      "          93       1.00      0.73      0.84        11\n",
      "          94       0.88      1.00      0.93         7\n",
      "          95       1.00      1.00      1.00        10\n",
      "          96       1.00      1.00      1.00        10\n",
      "          97       1.00      0.92      0.96        12\n",
      "          98       1.00      1.00      1.00        10\n",
      "          99       1.00      1.00      1.00        10\n",
      "         100       1.00      1.00      1.00        14\n",
      "         101       0.00      0.00      0.00         7\n",
      "         102       0.00      0.00      0.00        12\n",
      "         103       0.50      0.12      0.20         8\n",
      "         104       0.00      0.00      0.00         7\n",
      "         105       0.00      0.00      0.00         9\n",
      "         106       0.00      0.00      0.00        11\n",
      "         107       1.00      0.22      0.36         9\n",
      "         108       0.25      0.11      0.15         9\n",
      "         109       0.25      0.12      0.17         8\n",
      "         110       0.52      1.00      0.68       227\n",
      "         111       1.00      1.00      1.00       159\n",
      "         112       1.00      1.00      1.00       201\n",
      "         113       1.00      1.00      1.00       176\n",
      "         114       1.00      1.00      1.00       169\n",
      "         115       1.00      1.00      1.00       168\n",
      "         116       0.99      1.00      1.00       150\n",
      "         117       1.00      1.00      1.00       178\n",
      "         118       1.00      1.00      1.00       206\n",
      "         119       0.72      0.65      0.69        55\n",
      "         120       0.68      0.70      0.69        37\n",
      "         121       1.00      0.83      0.91        54\n",
      "         122       0.80      0.69      0.74        62\n",
      "         123       0.65      0.60      0.63        50\n",
      "         124       0.84      0.77      0.80        47\n",
      "         125       1.00      1.00      1.00        49\n",
      "         126       1.00      1.00      1.00       111\n",
      "         127       1.00      1.00      1.00        34\n",
      "         128       1.00      1.00      1.00       122\n",
      "         129       1.00      1.00      1.00        44\n",
      "         130       1.00      1.00      1.00        95\n",
      "         131       1.00      1.00      1.00        53\n",
      "         132       0.98      1.00      0.99       101\n",
      "         133       1.00      0.90      0.95        10\n",
      "         134       1.00      0.90      0.95        10\n",
      "         135       0.82      0.82      0.82        11\n",
      "         136       0.91      0.93      0.92        43\n",
      "         137       1.00      0.76      0.87        17\n",
      "         138       0.73      0.73      0.73        11\n",
      "         139       0.77      0.91      0.83        11\n",
      "         140       1.00      0.71      0.83        14\n",
      "         141       0.62      0.71      0.67         7\n",
      "         142       0.67      0.40      0.50        10\n",
      "         143       0.91      0.91      0.91        11\n",
      "         144       1.00      0.86      0.92         7\n",
      "         145       0.90      0.90      0.90        10\n",
      "         146       0.60      0.75      0.67         8\n",
      "         147       0.98      0.92      0.95        52\n",
      "         148       1.00      0.88      0.93         8\n",
      "         149       1.00      1.00      1.00        12\n",
      "         150       1.00      1.00      1.00        14\n",
      "         151       0.93      0.93      0.93        15\n",
      "         152       0.83      0.83      0.83         6\n",
      "         153       1.00      0.91      0.95        11\n",
      "         154       0.91      1.00      0.95        10\n",
      "         155       1.00      0.98      0.99        44\n",
      "         156       1.00      1.00      1.00        29\n",
      "         157       1.00      1.00      1.00        30\n",
      "         158       1.00      1.00      1.00        53\n",
      "         159       1.00      1.00      1.00        34\n",
      "         160       1.00      1.00      1.00        49\n",
      "         161       1.00      1.00      1.00         7\n",
      "         162       1.00      1.00      1.00        59\n",
      "         163       1.00      1.00      1.00        11\n",
      "         164       1.00      1.00      1.00        47\n",
      "         165       1.00      1.00      1.00        29\n",
      "         166       1.00      1.00      1.00        36\n",
      "         167       1.00      1.00      1.00        37\n",
      "         168       1.00      1.00      1.00        44\n",
      "         169       1.00      1.00      1.00        32\n",
      "         170       0.00      0.00      0.00         9\n",
      "         171       0.00      0.00      0.00         5\n",
      "         172       1.00      1.00      1.00        67\n",
      "         173       1.00      1.00      1.00        88\n",
      "         174       1.00      1.00      1.00        73\n",
      "         175       1.00      1.00      1.00        85\n",
      "         176       1.00      1.00      1.00       101\n",
      "         177       1.00      1.00      1.00        80\n",
      "         178       1.00      1.00      1.00        84\n",
      "         179       1.00      1.00      1.00        73\n",
      "         180       1.00      1.00      1.00        91\n",
      "         181       1.00      1.00      1.00        98\n",
      "         182       0.00      0.00      0.00        12\n",
      "         183       1.00      1.00      1.00        11\n",
      "         184       1.00      1.00      1.00        10\n",
      "         185       1.00      1.00      1.00        10\n",
      "         186       1.00      1.00      1.00         9\n",
      "         187       1.00      1.00      1.00        15\n",
      "         188       1.00      1.00      1.00        10\n",
      "         189       1.00      0.92      0.96        13\n",
      "         190       1.00      1.00      1.00        13\n",
      "         191       1.00      0.90      0.95        10\n",
      "         192       1.00      1.00      1.00        40\n",
      "         193       1.00      0.86      0.92        21\n",
      "         194       1.00      0.60      0.75        15\n",
      "         195       1.00      0.55      0.71        11\n",
      "         196       1.00      0.23      0.38        13\n",
      "         197       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.91      8280\n",
      "   macro avg       0.86      0.79      0.81      8280\n",
      "weighted avg       0.92      0.91      0.90      8280\n",
      "\n",
      "\n",
      "[[18  0  0 ...  0  0  0]\n",
      " [ 0 17  0 ...  0  0  0]\n",
      " [ 0  0 16 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  6  0  0]\n",
      " [ 0  0  0 ...  0  3  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#expected_y  = Y_test\n",
    "#predicted_y = model3.predict(X_test).round()\n",
    "#print(); print('LightGBM: ')\n",
    "#print(predicted_y)\n",
    "print(); print(metrics.classification_report(expected_y, predicted_y))\n",
    "print(); print(metrics.confusion_matrix(expected_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model3.predict_proba(test)\n",
    "submission = pd.DataFrame(data=pred)\n",
    "submission.index = test.index\n",
    "submission.index.name = 'id'\n",
    "submission = submission.sort_index()\n",
    "submission = submission.groupby('id').mean().round(6)\n",
    "submission.to_csv('submission_3.csv', index=True) #제출 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973686</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.003353</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "id                                                                           \n",
       "828   0.000094  0.000092  0.000096  0.000133  0.000141  0.000152  0.000139   \n",
       "829   0.000741  0.000730  0.000760  0.001049  0.001116  0.001202  0.001111   \n",
       "830   0.000528  0.000510  0.000537  0.000741  0.000789  0.000850  0.000783   \n",
       "831   0.000548  0.000534  0.000563  0.000776  0.001340  0.000889  0.000811   \n",
       "832   0.000714  0.000526  0.000549  0.001000  0.000861  0.000988  0.000850   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1543  0.000996  0.000999  0.001135  0.001431  0.001754  0.002355  0.001927   \n",
       "1544  0.000060  0.000059  0.000061  0.000084  0.000090  0.000097  0.000090   \n",
       "1545  0.000553  0.000541  0.000567  0.000782  0.000832  0.000897  0.000824   \n",
       "1546  0.000558  0.000517  0.000544  0.000751  0.000799  0.000861  0.000785   \n",
       "1547  0.000622  0.000596  0.000628  0.000866  0.000921  0.000993  0.000906   \n",
       "\n",
       "           7         8         9    ...       188       189       190  \\\n",
       "id                                  ...                                 \n",
       "828   0.000144  0.000595  0.000140  ...  0.000049  0.000045  0.000045   \n",
       "829   0.001142  0.004683  0.001106  ...  0.000385  0.000356  0.000356   \n",
       "830   0.000813  0.003310  0.000782  ...  0.000272  0.000252  0.000252   \n",
       "831   0.000840  0.003465  0.000818  ...  0.000285  0.000264  0.000264   \n",
       "832   0.000788  0.003137  0.001647  ...  0.000258  0.000239  0.000239   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1543  0.001604  0.006157  0.002425  ...  0.000506  0.000468  0.000468   \n",
       "1544  0.000092  0.000377  0.000089  ...  0.973686  0.000029  0.000029   \n",
       "1545  0.000851  0.003493  0.000825  ...  0.000287  0.000266  0.000266   \n",
       "1546  0.000824  0.003353  0.000792  ...  0.000276  0.000255  0.000255   \n",
       "1547  0.000940  0.003868  0.000914  ...  0.000318  0.000294  0.000294   \n",
       "\n",
       "           191       192       193       194       195       196       197  \n",
       "id                                                                          \n",
       "828   0.000049  0.000134  0.000096  0.000052  0.000049  0.000046  0.000074  \n",
       "829   0.000385  0.001058  0.000794  0.000414  0.000382  0.000360  0.000430  \n",
       "830   0.000272  0.000748  0.000559  0.000242  0.000273  0.000260  0.000315  \n",
       "831   0.000285  0.000783  0.000620  0.000257  0.000301  0.000264  0.000324  \n",
       "832   0.000258  0.000709  0.000533  0.000232  0.000265  0.000263  0.000302  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1543  0.000506  0.001391  0.001055  0.000489  0.000509  0.000478  0.000929  \n",
       "1544  0.000031  0.000085  0.000391  0.000030  0.000034  0.000031  0.000037  \n",
       "1545  0.000287  0.000789  0.000599  0.000266  0.000291  0.000276  0.000341  \n",
       "1546  0.000276  0.000758  0.000550  0.000246  0.000270  0.000256  0.000317  \n",
       "1547  0.000318  0.000874  0.000680  0.000289  0.000310  0.000337  0.000350  \n",
       "\n",
       "[720 rows x 198 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
