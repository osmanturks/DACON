{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt # 데이터 시각화\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import os\n",
    "import re\n",
    "import multiprocessing # 여러 개의 일꾼 (cpu)들에게 작업을 분산시키는 역할\n",
    "from multiprocessing import Pool \n",
    "from functools import partial # 함수가 받는 인자들 중 몇개를 고정 시켜서 새롭게 파생된 함수를 형성하는 역할\n",
    "from my_data_loader import my_data_loader \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (13,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "import lightgbm as lgb\n",
    "seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_label = pd.read_csv(\"train_label.csv\")\n",
    "plt.hist(train_label['label'], bins=len(train_label['label'].unique()))\n",
    "plt.xlim(0,200)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('# of label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체적으로 Imbalance Data, 1개뿐인 라벨이 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad, CLOSE, Equip Fail, No Data, Normal, OFF, ON, OPEN, System.Char[] 등 문자열 데이터를 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "realData1 = \"Train/30.csv\"\n",
    "realData2 = \"Test/1154.csv\"\n",
    "realData3 = \"Test/1168.csv\"\n",
    "realData4 = \"additinal_data/additinal_data1\"\n",
    "realData5 = \"additinal_data/additinal_data2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'train/'\n",
    "test_folder = 'test/'\n",
    "train_label_path = 'train_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = os.listdir(train_folder)\n",
    "test_list = os.listdir(test_folder)\n",
    "train_label = pd.read_csv(train_label_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataPreprocessn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 csv 파일의 상태_B로 변화는 시점이 1~15초 랜덤으로 가정\n",
    "def my_data_loader_all(func, files, folder='', train_label=None, event_time=10, nrows=60):   \n",
    "    func_fixed = partial(func, folder=folder, train_label=train_label, event_time=event_time, nrows=nrows)     \n",
    "    if __name__ == '__main__':\n",
    "        pool = Pool(processes=multiprocessing.cpu_count()) \n",
    "        df_list = list(pool.imap(func_fixed, files)) \n",
    "        pool.close()\n",
    "        pool.join()        \n",
    "    combined_df = pd.concat(df_list)    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = my_data_loader_all(my_data_loader, train_list, folder=train_folder, train_label=train_label, event_time=10, nrows=60)\n",
    "train = pd.read_csv(\"train_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 test 셋 데이터에 대해서 1~15초부터 랜덤하게 상태_B가 시작된다고 가정 \n",
    "#test = my_data_loader_all(my_data_loader, test_list, folder=test_folder, train_label=None, event_time=10, nrows=60)\n",
    "test = pd.read_csv(\"test_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv(\"train_v1.csv\")\n",
    "#test.to_csv(\"test_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train.copy()\n",
    "train_y = train_1['label']\n",
    "train_1 = train_1.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0      V0000     V0001     V0002     V0003     V0004  \\\n",
      "15818         645  30.469176  8.680222  8.718171  8.727309  8.694180   \n",
      "5290          369  30.474732  8.794888  8.708693  8.693407  8.725450   \n",
      "16265         269  30.483363  8.677149  8.691058  8.661279  8.733543   \n",
      "11910         346  30.468302  8.704963  8.708931  8.711129  8.701583   \n",
      "33893         821  30.482572  8.817756  8.719177  8.820185  8.711986   \n",
      "...           ...        ...       ...       ...       ...       ...   \n",
      "5520          816  30.472774  8.675617  8.714310  8.704230  8.673045   \n",
      "35814         771  30.469492  8.733404  8.709039  8.694300  8.693976   \n",
      "20463         482  30.465643  8.823523  8.729735  8.694199  8.612927   \n",
      "18638         732  30.474946  8.764901  8.710119  8.660449  8.689067   \n",
      "35683         567  30.464810  8.756298  8.691154  8.698040  8.695485   \n",
      "\n",
      "            V0005       V0006         V0007  V0008  ...  V5111  V5112  V5113  \\\n",
      "15818  212.117293  243.249118 -2.447466e-19    0.0  ...    1.0    1.0    1.0   \n",
      "5290   225.000127  211.019003  3.376461e-20    0.0  ...    1.0    1.0    1.0   \n",
      "16265  201.370566  216.420082  6.868010e-19    0.0  ...    1.0    1.0    1.0   \n",
      "11910  200.046995  210.656507  1.643456e-19    0.0  ...    1.0    1.0    1.0   \n",
      "33893  182.719561  189.625599 -2.407155e-19    0.0  ...    1.0    1.0    1.0   \n",
      "...           ...         ...           ...    ...  ...    ...    ...    ...   \n",
      "5520   216.448977  186.396417  1.261673e-19    0.0  ...    1.0    1.0    1.0   \n",
      "35814  165.711850  189.610694  4.935999e-19    0.0  ...    1.0    1.0    1.0   \n",
      "20463  180.081893  194.605741  6.069726e-19    0.0  ...    1.0    1.0    1.0   \n",
      "18638  173.176796  154.197991  2.903507e-19    0.0  ...    1.0    1.0    1.0   \n",
      "35683  252.061816  259.914651 -6.155002e-20    0.0  ...    1.0    1.0    1.0   \n",
      "\n",
      "       V5114  V5115  V5116  V5117         V5118  V5119  V5120  \n",
      "15818    1.0   60.0    0.0    0.0  6.485913e-07   85.4    0.0  \n",
      "5290     1.0   60.0    0.0    0.0 -3.732335e-06   85.4    0.0  \n",
      "16265    1.0   60.0    0.0    0.0  2.069969e-05   85.4    0.0  \n",
      "11910    1.0   60.0    0.0    0.0 -1.756999e-05   85.4    0.0  \n",
      "33893    1.0   60.0    0.0    0.0  4.064788e-06   85.4    0.0  \n",
      "...      ...    ...    ...    ...           ...    ...    ...  \n",
      "5520     1.0   60.0    0.0    0.0 -8.613318e-06   85.4    0.0  \n",
      "35814    1.0   60.0    0.0    0.0  3.588538e-06   85.4    0.0  \n",
      "20463    1.0   60.0    0.0    0.0 -2.094340e-05   85.4    0.0  \n",
      "18638    1.0   60.0    0.0    0.0 -6.342063e-07   85.4    0.0  \n",
      "35683    1.0   60.0    0.0    0.0  1.205339e-05   85.4    0.0  \n",
      "\n",
      "[34405 rows x 5122 columns]        Unnamed: 0      V0000      V0001      V0002      V0003      V0004  \\\n",
      "21803         523  30.469546   8.667132   8.713942   8.705532   8.693773   \n",
      "28363         713  30.435837   8.626891   8.696924   8.704857   8.698347   \n",
      "10255         556  30.464690  11.253033  11.133864  11.144452  11.150019   \n",
      "15746         679  30.470522   8.638634   8.708793   8.704119   8.717553   \n",
      "39672         198  30.454065   8.746609   8.692978   8.652676   8.680136   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "34061         362  30.481832   8.695434   8.653131   8.683216   8.708494   \n",
      "8132          184  30.454755   8.676950   8.697092   8.667489   8.730514   \n",
      "31791         310  30.482378   8.699306   8.697977   8.738060   8.693091   \n",
      "24599         275  30.466090   8.763907   8.708514   8.751982   8.713516   \n",
      "10223         230  30.496440   8.675345   8.696929   8.714599   8.736295   \n",
      "\n",
      "            V0005       V0006         V0007  V0008  ...  V5111  V5112  V5113  \\\n",
      "21803  182.596392  179.444513 -1.190788e-19    0.0  ...    1.0    1.0    1.0   \n",
      "28363  169.195547  188.749141 -4.240423e-19    0.0  ...    1.0    1.0    1.0   \n",
      "10255  195.335403  177.476003  2.282327e-19    0.0  ...    1.0    1.0    1.0   \n",
      "15746  204.624602  156.732687 -7.633787e-20    0.0  ...    1.0    1.0    1.0   \n",
      "39672  182.369475  170.927145 -2.216142e-19    0.0  ...    1.0    1.0    1.0   \n",
      "...           ...         ...           ...    ...  ...    ...    ...    ...   \n",
      "34061  212.568149  158.453134 -2.194264e-19    0.0  ...    1.0    1.0    1.0   \n",
      "8132   208.360699  147.083429 -3.541562e-19    0.0  ...    1.0    1.0    1.0   \n",
      "31791  205.746123  205.657030 -7.432924e-20    0.0  ...    1.0    1.0    1.0   \n",
      "24599  176.236241  222.436258  1.417557e-19    0.0  ...    1.0    1.0    1.0   \n",
      "10223  197.105334  194.295607 -1.625289e-19    0.0  ...    1.0    1.0    1.0   \n",
      "\n",
      "       V5114  V5115  V5116  V5117     V5118  V5119  V5120  \n",
      "21803    1.0   60.0    0.0    0.0  0.000009   85.4    0.0  \n",
      "28363    1.0   60.0    0.0    0.0 -0.000014   85.4    0.0  \n",
      "10255    1.0   60.0    0.0    0.0  0.000023   85.4    0.0  \n",
      "15746    1.0   60.0    0.0    0.0  0.000012   85.4    0.0  \n",
      "39672    1.0   60.0    0.0    0.0  0.000012   85.4    0.0  \n",
      "...      ...    ...    ...    ...       ...    ...    ...  \n",
      "34061    1.0   60.0    0.0    0.0  0.000011   85.4    0.0  \n",
      "8132     1.0   60.0    0.0    0.0  0.000009   85.4    0.0  \n",
      "31791    1.0   60.0    0.0    0.0  0.000021   85.4    0.0  \n",
      "24599    1.0   60.0    0.0    0.0 -0.000012   85.4    0.0  \n",
      "10223    1.0   60.0    0.0    0.0  0.000021   85.4    0.0  \n",
      "\n",
      "[8602 rows x 5122 columns] 15818      0\n",
      "5290     141\n",
      "16265     85\n",
      "11910    157\n",
      "33893    137\n",
      "        ... \n",
      "5520     178\n",
      "35814    147\n",
      "20463     27\n",
      "18638    125\n",
      "35683      2\n",
      "Name: label, Length: 34405, dtype: int64 21803    122\n",
      "28363    176\n",
      "10255     28\n",
      "15746    100\n",
      "39672    110\n",
      "        ... \n",
      "34061      8\n",
      "8132      79\n",
      "31791     18\n",
      "24599     75\n",
      "10223    106\n",
      "Name: label, Length: 8602, dtype: int64\n",
      "(34405, 5122) (8602, 5122) (34405,) (8602,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(train_1, train_y, test_size=0.2, random_state=seed)\n",
    "print(X_train,X_test,Y_train,Y_test)\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClaasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=-1, oob_score=False, random_state=0, verbose=1,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=0, verbose=1, n_jobs=-1)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        20\n",
      "           1       1.00      0.47      0.64        17\n",
      "           2       0.95      0.82      0.88        22\n",
      "           3       0.91      0.80      0.85        25\n",
      "           4       0.96      0.71      0.82        38\n",
      "           5       1.00      0.85      0.92        41\n",
      "           6       1.00      0.92      0.96        37\n",
      "           7       1.00      0.93      0.96        28\n",
      "           8       0.68      0.92      0.78       125\n",
      "           9       0.23      0.11      0.15        28\n",
      "          10       0.33      0.75      0.46        32\n",
      "          11       0.00      0.00      0.00        22\n",
      "          12       0.88      0.28      0.42        25\n",
      "          13       1.00      0.06      0.11        35\n",
      "          14       0.25      0.27      0.26        30\n",
      "          15       0.60      0.56      0.58        16\n",
      "          16       0.40      0.29      0.33        35\n",
      "          17       0.72      0.95      0.82       216\n",
      "          18       0.99      0.97      0.98       105\n",
      "          19       0.80      0.84      0.82        51\n",
      "          20       0.71      0.43      0.54        23\n",
      "          21       0.98      1.00      0.99        56\n",
      "          22       1.00      0.97      0.99        40\n",
      "          23       0.85      0.88      0.86        76\n",
      "          24       1.00      0.86      0.93        22\n",
      "          25       1.00      0.83      0.91        30\n",
      "          26       1.00      0.93      0.96        27\n",
      "          27       1.00      0.89      0.94        28\n",
      "          28       0.88      0.93      0.91       180\n",
      "          29       0.94      0.89      0.92        57\n",
      "          30       1.00      0.96      0.98        56\n",
      "          31       1.00      1.00      1.00         7\n",
      "          32       0.86      0.86      0.86         7\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.93      0.88      0.90        16\n",
      "          35       1.00      0.92      0.96        12\n",
      "          36       1.00      0.27      0.43        11\n",
      "          37       1.00      0.67      0.80        12\n",
      "          38       0.85      0.96      0.90       183\n",
      "          39       1.00      0.89      0.94         9\n",
      "          40       0.98      0.96      0.97        85\n",
      "          41       0.92      1.00      0.96        11\n",
      "          42       0.86      0.75      0.80         8\n",
      "          43       0.83      0.91      0.87        11\n",
      "          44       1.00      0.81      0.90        16\n",
      "          45       0.89      0.89      0.89         9\n",
      "          46       0.88      0.88      0.88         8\n",
      "          47       1.00      0.91      0.95        11\n",
      "          48       0.64      0.78      0.70         9\n",
      "          49       0.83      0.91      0.87        11\n",
      "          50       1.00      0.67      0.80         6\n",
      "          51       0.90      1.00      0.95         9\n",
      "          52       0.73      0.89      0.80         9\n",
      "          53       0.98      0.98      0.98        54\n",
      "          54       0.98      0.93      0.95       112\n",
      "          55       0.80      0.40      0.53        10\n",
      "          56       0.25      0.08      0.12        12\n",
      "          57       0.33      0.08      0.12        13\n",
      "          58       1.00      0.73      0.85        15\n",
      "          59       0.50      0.18      0.27        11\n",
      "          60       0.62      0.50      0.56        10\n",
      "          61       1.00      1.00      1.00        43\n",
      "          62       1.00      0.98      0.99        55\n",
      "          63       0.67      0.58      0.62        31\n",
      "          64       0.69      0.66      0.68        38\n",
      "          65       0.88      0.50      0.64        42\n",
      "          66       0.56      0.96      0.71        23\n",
      "          67       0.47      0.82      0.60        38\n",
      "          68       1.00      0.98      0.99       131\n",
      "          69       0.91      0.92      0.92       157\n",
      "          70       1.00      0.77      0.87        13\n",
      "          71       0.00      0.00      0.00        10\n",
      "          72       0.75      0.50      0.60         6\n",
      "          73       0.90      0.50      0.64        18\n",
      "          74       0.75      0.30      0.43        10\n",
      "          75       0.95      0.89      0.92        83\n",
      "          76       0.87      0.93      0.90        80\n",
      "          77       0.29      0.32      0.31        87\n",
      "          78       0.97      0.90      0.94        40\n",
      "          79       0.81      0.63      0.71        35\n",
      "          80       0.88      0.27      0.41        26\n",
      "          81       1.00      0.52      0.68        27\n",
      "          82       1.00      0.60      0.75        15\n",
      "          83       0.32      0.79      0.46       105\n",
      "          84       1.00      0.95      0.98        22\n",
      "          85       0.92      0.92      0.92        73\n",
      "          86       1.00      0.97      0.98        33\n",
      "          87       1.00      0.88      0.94        25\n",
      "          88       1.00      0.75      0.86        28\n",
      "          89       1.00      0.90      0.95        20\n",
      "          90       1.00      0.85      0.92        26\n",
      "          91       0.68      0.70      0.69       105\n",
      "          92       1.00      0.93      0.96        14\n",
      "          93       1.00      0.82      0.90        11\n",
      "          94       1.00      0.80      0.89        10\n",
      "          95       1.00      1.00      1.00        11\n",
      "          96       1.00      1.00      1.00         8\n",
      "          97       1.00      1.00      1.00         6\n",
      "          98       1.00      0.75      0.86        12\n",
      "          99       1.00      1.00      1.00         6\n",
      "         100       0.92      0.92      0.92        12\n",
      "         101       1.00      0.22      0.36         9\n",
      "         102       0.00      0.00      0.00        10\n",
      "         103       0.00      0.00      0.00        18\n",
      "         104       0.60      0.33      0.43         9\n",
      "         105       0.67      0.14      0.24        14\n",
      "         106       1.00      0.11      0.20         9\n",
      "         107       1.00      0.70      0.82        10\n",
      "         108       0.00      0.00      0.00        12\n",
      "         109       1.00      0.33      0.50        12\n",
      "         110       0.79      0.95      0.86       242\n",
      "         111       0.85      0.93      0.89       192\n",
      "         112       0.92      0.95      0.93       192\n",
      "         113       0.97      0.94      0.95       201\n",
      "         114       0.80      0.94      0.87       206\n",
      "         115       0.98      0.97      0.98       176\n",
      "         116       1.00      0.98      0.99       181\n",
      "         117       1.00      0.95      0.97       182\n",
      "         118       0.86      0.93      0.90       235\n",
      "         119       0.63      0.56      0.59        59\n",
      "         120       0.65      0.59      0.62        44\n",
      "         121       0.84      0.80      0.82        45\n",
      "         122       0.65      0.59      0.62        69\n",
      "         123       0.53      0.57      0.55        61\n",
      "         124       0.90      0.70      0.79        61\n",
      "         125       0.98      0.93      0.95        55\n",
      "         126       0.93      0.95      0.94        93\n",
      "         127       0.95      0.97      0.96        39\n",
      "         128       0.86      0.93      0.89       117\n",
      "         129       0.94      0.92      0.93        49\n",
      "         130       0.90      0.84      0.87       115\n",
      "         131       0.91      0.93      0.92        42\n",
      "         132       0.74      0.97      0.83        89\n",
      "         133       1.00      0.62      0.77         8\n",
      "         134       1.00      0.92      0.96        12\n",
      "         135       1.00      0.78      0.88         9\n",
      "         136       0.97      0.83      0.89        46\n",
      "         137       1.00      0.83      0.91        12\n",
      "         138       1.00      0.91      0.95        11\n",
      "         139       1.00      0.73      0.85        15\n",
      "         140       1.00      0.58      0.74        12\n",
      "         141       1.00      0.91      0.95        11\n",
      "         142       0.60      0.50      0.55         6\n",
      "         143       1.00      0.64      0.78        14\n",
      "         144       1.00      0.46      0.63        13\n",
      "         145       0.88      0.58      0.70        12\n",
      "         146       1.00      0.83      0.91        12\n",
      "         147       0.85      0.83      0.84        47\n",
      "         148       1.00      0.58      0.74        12\n",
      "         149       0.80      1.00      0.89        12\n",
      "         150       1.00      0.50      0.67        12\n",
      "         151       1.00      0.70      0.82        10\n",
      "         152       1.00      0.64      0.78        11\n",
      "         153       1.00      0.67      0.80         9\n",
      "         154       1.00      0.70      0.82        10\n",
      "         155       0.87      0.89      0.88        38\n",
      "         156       0.92      0.90      0.91        39\n",
      "         157       0.94      0.88      0.91        33\n",
      "         158       0.85      0.86      0.85        58\n",
      "         159       1.00      0.82      0.90        17\n",
      "         160       0.94      0.82      0.87        55\n",
      "         161       1.00      0.73      0.85        15\n",
      "         162       0.41      0.92      0.57        60\n",
      "         163       1.00      1.00      1.00        10\n",
      "         164       0.94      0.89      0.91        53\n",
      "         165       0.90      0.66      0.76        41\n",
      "         166       0.78      0.84      0.81        45\n",
      "         167       0.95      0.97      0.96        40\n",
      "         168       0.46      0.82      0.59        50\n",
      "         169       0.65      0.56      0.60        43\n",
      "         170       0.00      0.00      0.00        10\n",
      "         171       0.40      0.25      0.31         8\n",
      "         172       0.93      0.92      0.93        77\n",
      "         173       0.98      0.91      0.94        96\n",
      "         174       0.99      0.89      0.94        93\n",
      "         175       1.00      0.96      0.98        77\n",
      "         176       0.98      0.90      0.94        93\n",
      "         177       0.95      0.91      0.93        89\n",
      "         178       1.00      0.98      0.99        83\n",
      "         179       1.00      0.99      0.99        79\n",
      "         180       1.00      0.92      0.96        83\n",
      "         181       0.93      0.93      0.93        83\n",
      "         182       0.00      0.00      0.00        12\n",
      "         183       1.00      1.00      1.00        11\n",
      "         184       0.92      1.00      0.96        11\n",
      "         185       1.00      0.75      0.86         4\n",
      "         186       1.00      1.00      1.00         5\n",
      "         187       1.00      0.85      0.92        13\n",
      "         188       1.00      1.00      1.00        11\n",
      "         189       1.00      1.00      1.00         7\n",
      "         190       1.00      0.67      0.80        15\n",
      "         191       1.00      1.00      1.00        10\n",
      "         192       0.91      0.91      0.91        23\n",
      "         193       1.00      0.74      0.85        23\n",
      "         194       1.00      0.62      0.76        13\n",
      "         195       1.00      0.50      0.67        10\n",
      "         196       0.45      0.71      0.56         7\n",
      "         197       1.00      0.07      0.12        15\n",
      "\n",
      "    accuracy                           0.84      8602\n",
      "   macro avg       0.85      0.74      0.77      8602\n",
      "weighted avg       0.86      0.84      0.84      8602\n",
      "\n",
      "\n",
      "[[16  0  0 ...  0  0  0]\n",
      " [ 0  8  1 ...  0  0  0]\n",
      " [ 0  0 18 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  5  1  0]\n",
      " [ 0  0  0 ...  0  5  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "expected_y  = Y_test\n",
    "predicted_y = model.predict(X_test).round()\n",
    "print(); print('RandomForestClassifier: ')\n",
    "print(); print(metrics.classification_report(expected_y, predicted_y))\n",
    "print(); print(metrics.confusion_matrix(expected_y, predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "X_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n",
    "\n",
    "train_ds = lgb.Dataset(X_train, label = Y_train)\n",
    "test_ds = lgb.Dataset(X_test, label = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "#model.fit(X_train,Y_train)\n",
    "lgb_params = {\n",
    "        'objective':'multiclass',\n",
    "        'boosting_type':'gbdt', #gradient boosting decision tree\n",
    "        'metric':'multi_logloss',\n",
    "        'num_class':198,\n",
    "        'learning_rate':0.02,\n",
    "        'seed': 5,\n",
    "        'n_jobs': -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 3.28624\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's multi_logloss: 3.02749\n",
      "[3]\tvalid_0's multi_logloss: 2.84508\n",
      "[4]\tvalid_0's multi_logloss: 2.69981\n",
      "[5]\tvalid_0's multi_logloss: 2.57696\n",
      "[6]\tvalid_0's multi_logloss: 2.47148\n",
      "[7]\tvalid_0's multi_logloss: 2.37878\n",
      "[8]\tvalid_0's multi_logloss: 2.29634\n",
      "[9]\tvalid_0's multi_logloss: 2.22167\n",
      "[10]\tvalid_0's multi_logloss: 2.15358\n",
      "[11]\tvalid_0's multi_logloss: 2.09138\n",
      "[12]\tvalid_0's multi_logloss: 2.03358\n",
      "[13]\tvalid_0's multi_logloss: 1.97991\n",
      "[14]\tvalid_0's multi_logloss: 1.92964\n",
      "[15]\tvalid_0's multi_logloss: 1.88244\n",
      "[16]\tvalid_0's multi_logloss: 1.83787\n",
      "[17]\tvalid_0's multi_logloss: 1.79593\n",
      "[18]\tvalid_0's multi_logloss: 1.75591\n",
      "[19]\tvalid_0's multi_logloss: 1.718\n",
      "[20]\tvalid_0's multi_logloss: 1.68209\n",
      "[21]\tvalid_0's multi_logloss: 1.64778\n",
      "[22]\tvalid_0's multi_logloss: 1.61461\n",
      "[23]\tvalid_0's multi_logloss: 1.58296\n",
      "[24]\tvalid_0's multi_logloss: 1.55287\n",
      "[25]\tvalid_0's multi_logloss: 1.5239\n",
      "[26]\tvalid_0's multi_logloss: 1.49611\n",
      "[27]\tvalid_0's multi_logloss: 1.46939\n",
      "[28]\tvalid_0's multi_logloss: 1.4435\n",
      "[29]\tvalid_0's multi_logloss: 1.41856\n",
      "[30]\tvalid_0's multi_logloss: 1.39445\n",
      "[31]\tvalid_0's multi_logloss: 1.37117\n",
      "[32]\tvalid_0's multi_logloss: 1.34856\n",
      "[33]\tvalid_0's multi_logloss: 1.32662\n",
      "[34]\tvalid_0's multi_logloss: 1.30536\n",
      "[35]\tvalid_0's multi_logloss: 1.28497\n",
      "[36]\tvalid_0's multi_logloss: 1.26503\n",
      "[37]\tvalid_0's multi_logloss: 1.24566\n",
      "[38]\tvalid_0's multi_logloss: 1.22686\n",
      "[39]\tvalid_0's multi_logloss: 1.20864\n",
      "[40]\tvalid_0's multi_logloss: 1.19093\n",
      "[41]\tvalid_0's multi_logloss: 1.17367\n",
      "[42]\tvalid_0's multi_logloss: 1.15688\n",
      "[43]\tvalid_0's multi_logloss: 1.14054\n",
      "[44]\tvalid_0's multi_logloss: 1.12466\n",
      "[45]\tvalid_0's multi_logloss: 1.10934\n",
      "[46]\tvalid_0's multi_logloss: 1.0943\n",
      "[47]\tvalid_0's multi_logloss: 1.07961\n",
      "[48]\tvalid_0's multi_logloss: 1.06532\n",
      "[49]\tvalid_0's multi_logloss: 1.0513\n",
      "[50]\tvalid_0's multi_logloss: 1.03776\n",
      "[51]\tvalid_0's multi_logloss: 1.02442\n",
      "[52]\tvalid_0's multi_logloss: 1.0114\n",
      "[53]\tvalid_0's multi_logloss: 0.998729\n",
      "[54]\tvalid_0's multi_logloss: 0.986405\n",
      "[55]\tvalid_0's multi_logloss: 0.974285\n",
      "[56]\tvalid_0's multi_logloss: 0.962444\n",
      "[57]\tvalid_0's multi_logloss: 0.950977\n",
      "[58]\tvalid_0's multi_logloss: 0.93965\n",
      "[59]\tvalid_0's multi_logloss: 0.928659\n",
      "[60]\tvalid_0's multi_logloss: 0.917885\n",
      "[61]\tvalid_0's multi_logloss: 0.907379\n",
      "[62]\tvalid_0's multi_logloss: 0.89703\n",
      "[63]\tvalid_0's multi_logloss: 0.886962\n",
      "[64]\tvalid_0's multi_logloss: 0.877121\n",
      "[65]\tvalid_0's multi_logloss: 0.867564\n",
      "[66]\tvalid_0's multi_logloss: 0.858152\n",
      "[67]\tvalid_0's multi_logloss: 0.848929\n",
      "[68]\tvalid_0's multi_logloss: 0.839946\n",
      "[69]\tvalid_0's multi_logloss: 0.831047\n",
      "[70]\tvalid_0's multi_logloss: 0.82244\n",
      "[71]\tvalid_0's multi_logloss: 0.813973\n",
      "[72]\tvalid_0's multi_logloss: 0.80574\n",
      "[73]\tvalid_0's multi_logloss: 0.797593\n",
      "[74]\tvalid_0's multi_logloss: 0.789541\n",
      "[75]\tvalid_0's multi_logloss: 0.781708\n",
      "[76]\tvalid_0's multi_logloss: 0.774058\n",
      "[77]\tvalid_0's multi_logloss: 0.766586\n",
      "[78]\tvalid_0's multi_logloss: 0.759244\n",
      "[79]\tvalid_0's multi_logloss: 0.752037\n",
      "[80]\tvalid_0's multi_logloss: 0.744956\n",
      "[81]\tvalid_0's multi_logloss: 0.73802\n",
      "[82]\tvalid_0's multi_logloss: 0.731242\n",
      "[83]\tvalid_0's multi_logloss: 0.724641\n",
      "[84]\tvalid_0's multi_logloss: 0.718158\n",
      "[85]\tvalid_0's multi_logloss: 0.71182\n",
      "[86]\tvalid_0's multi_logloss: 0.705539\n",
      "[87]\tvalid_0's multi_logloss: 0.699301\n",
      "[88]\tvalid_0's multi_logloss: 0.693311\n",
      "[89]\tvalid_0's multi_logloss: 0.687361\n",
      "[90]\tvalid_0's multi_logloss: 0.681586\n",
      "[91]\tvalid_0's multi_logloss: 0.675867\n",
      "[92]\tvalid_0's multi_logloss: 0.670232\n",
      "[93]\tvalid_0's multi_logloss: 0.664805\n",
      "[94]\tvalid_0's multi_logloss: 0.659511\n",
      "[95]\tvalid_0's multi_logloss: 0.654218\n",
      "[96]\tvalid_0's multi_logloss: 0.648982\n",
      "[97]\tvalid_0's multi_logloss: 0.643908\n",
      "[98]\tvalid_0's multi_logloss: 0.638872\n",
      "[99]\tvalid_0's multi_logloss: 0.634006\n",
      "[100]\tvalid_0's multi_logloss: 0.629181\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.629181\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(lgb_params, train_ds, 100, test_ds, verbose_eval=True, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgbmodel.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'lgbmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('lgbmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-12e00e86e7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlgb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpred_contrib\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mcontributions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \"\"\"\n\u001b[0;32m--> 862\u001b[0;31m         result = super(LGBMClassifier, self).predict(X, raw_score, num_iteration,\n\u001b[0m\u001b[1;32m    863\u001b[0m                                                      pred_leaf, pred_contrib, **kwargs)\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_classes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mraw_score\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "model2 = lgb.LGBMClassifier.predict_proba(lgb_params, train_ds, 100, test_ds, verbose_eval=True, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM: \n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "expected_y  = Y_test\n",
    "predicted_y = model.predict(X_test).round()\n",
    "print(); print('LightGBM: ')\n",
    "print(predicted_y)\n",
    "#print(); print(metrics.classification_report(expected_y, predicted_y))\n",
    "#print(); print(metrics.confusion_matrix(expected_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Booster' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-21a8e40692fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Booster' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "pred = model.predict_proba(test)\n",
    "submission = pd.DataFrame(data=pred)\n",
    "submission.index = test.index\n",
    "submission.index.name = 'id'\n",
    "submission = submission.sort_index()\n",
    "submission = submission.groupby('id').mean()\n",
    "submission.to_csv('submission_1.csv', index=True) #제출 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
